{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ccae883-1904-48a6-81b3-d0228caeab1f",
   "metadata": {},
   "source": [
    "### Multivariate time series prediction using MLP with Hyperparameter optimization\n",
    "\n",
    "\n",
    "\n",
    "In this notebook, we use **Optuna** to find the optimum values of hyperparameters. Optuna is a package for optimizing hyperparameters xxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4117e9d0-7925-4788-bbe1-28c60c80bb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cuser/Documents/cybertraining/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8af4f85b-3822-4288-9566-0f0ac0cfa053",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../dataset/final_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc9eb646-4308-449c-84c0-00cd1de182c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Implement determinism. Set a fixed value for random seed so that when the parameters are initialized, they are initialized same across all experiments.\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# If you are using CUDA, also set the seed for it\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Set the seed for NumPy\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0600e784-ec1c-4ab8-ab6b-9b765b4dee83",
   "metadata": {},
   "source": [
    "Here we define **RiverData** a custom Dataset class to load the dataset we have. It extends the pytorch Dataset class.  \n",
    "- We need to define \\_\\_init__() function which can be used for loading data from file and optionally for data preprocessing.\n",
    "- Thereafter we define \\_\\_len__() function which gives the length of dataset.\n",
    "- Then we define \\_\\_getitem__() function which returns an instance of (feature, label) tuple which can be used for model training.\n",
    "  For our time series data, feature means the past values to be used for training and label means the future values to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "742b694d-9966-4fd3-bca7-51f398a8775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiverData(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, df, target, datecol, seq_len, pred_len):\n",
    "        self.df = df\n",
    "        self.datecol = datecol\n",
    "        self.target = target\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.setIndex()\n",
    "        \n",
    "\n",
    "    def setIndex(self):\n",
    "        self.df.set_index(self.datecol, inplace=True)\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) - self.seq_len - self.pred_len\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if len(self.df) <= (idx + self.seq_len+self.pred_len):\n",
    "            raise IndexError(f\"Index {idx} is out of bounds for dataset of size {len(self.df)}\")\n",
    "        df_piece = self.df[idx:idx+self.seq_len].values\n",
    "        feature = torch.tensor(df_piece, dtype=torch.float32)\n",
    "        label_piece = self.df[self.target][idx + self.seq_len:  idx+self.seq_len+self.pred_len].values\n",
    "        label = torch.tensor(label_piece, dtype=torch.float32)\n",
    "        return (feature.T, label) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c24959-e72f-435d-8a14-69a648fa5ab1",
   "metadata": {},
   "source": [
    "### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d897d9-946e-4bbe-90a6-cc8d8a223c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)\n",
    "df = df[df['DATE'] > '2014']\n",
    "raw_df = df.drop('DATE', axis=1, inplace=False)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply the transformations\n",
    "df_scaled = scaler.fit_transform(raw_df)\n",
    "\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=raw_df.columns)\n",
    "df_scaled['DATE'] = df['DATE']\n",
    "df = df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4781c6-88ed-4b28-aa0e-f669bf7c75f5",
   "metadata": {},
   "source": [
    "Some advanced python syntax have been used here. \\\n",
    "*common_args : it's used to pass arguments to a function, where common_args represents a python list \\\n",
    "**common_args: it's used to pass arguments to a function, where common_args represents a python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e63a535-a817-4429-be17-f3e3a02e4f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_size = int(0.7 * len(df))\n",
    "test_size = int(0.2 * len(df))\n",
    "val_size = len(df) - train_size - test_size\n",
    "\n",
    "seq_len = 13\n",
    "pred_len = 1\n",
    "num_features = 7\n",
    "\n",
    "common_args = ['gauge_height', 'DATE', seq_len, pred_len]\n",
    "train_dataset = RiverData(df[:train_size], *common_args)\n",
    "val_dataset = RiverData(df[train_size: train_size+val_size], *common_args)\n",
    "test_dataset = RiverData(df[train_size+val_size : len(df)], *common_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "501879b1-dd1c-4ff0-afb3-ff25a345df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important parameters\n",
    "\n",
    "BATCH_SIZE = 512 # keep as big as can be handled by GPU and memory\n",
    "SHUFFLE = False # we don't shuffle the time series data\n",
    "DATA_LOAD_WORKERS = 1 # it depends on amount of data you need to load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae359aec-9435-4229-8587-5f120b0370b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "common_args = {'batch_size': BATCH_SIZE, 'shuffle': SHUFFLE}\n",
    "train_loader = DataLoader(train_dataset, **common_args)\n",
    "val_loader = DataLoader(val_dataset, **common_args)\n",
    "test_loader = DataLoader(test_dataset, **common_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ac35c-8ecc-4ab7-a97c-0cb4d59bc624",
   "metadata": {},
   "source": [
    "### Here we define our pytorch model.\n",
    "\n",
    "BasicMLPNetwork is the model class, it extends the Module class provided by pytorch. \\\n",
    "- We define \\_\\_init__() function. It sets up layers and defines the model parameters.\n",
    "- Also, we define forward() function which defines how the forwared pass computation occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9b72d79-65b6-4fd0-9d0b-025c2a0fc293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are adding dropout layers.\n",
    "\n",
    "class BasicMLPNetwork(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len, pred_len, num_features, dropout):\n",
    "        # call the constructor of the base class\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.num_features = num_features\n",
    "        hidden_size_time = 512\n",
    "        hidden_size_feat = 128\n",
    "        # define layers for combining across time series\n",
    "        self.fc1 = torch.nn.Linear(self.seq_len, hidden_size_time)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout1 = torch.nn.Dropout(p=dropout)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size_time, self.pred_len)\n",
    "        self.dropout2 = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        # define layers for combining across the features\n",
    "        self.fc3 = torch.nn.Linear(self.num_features, hidden_size_feat)\n",
    "        self.fc4 = torch.nn.Linear(hidden_size_feat, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # computation over time\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out) # has dimension 512 x 7 x 12\n",
    "        out = self.dropout2(out)\n",
    "        # computation over features\n",
    "        out = out.transpose(1,2) # dimension 512 x 12 x 7\n",
    "        out = self.fc3(out) # dimension 512 x 12 x 20\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out) # dimension 512 x 12 x 1\n",
    "\n",
    "        out = out.squeeze(-1) # dimension 512 x 12\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Note that the gradients are stored insize the FC layer objects\n",
    "# For each training example we need to get rid of these gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5277794b-fe37-4595-8554-d26db5710e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1afde643-9953-4129-8415-7b4836c31300",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b3cc7a8-9844-4eae-a601-4787513eb1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape:  torch.Size([512, 7, 8])\n",
      "labels shape:  torch.Size([512, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, (f,l) in enumerate(train_loader):\n",
    "    print('features shape: ', f.shape)\n",
    "    print('labels shape: ', l.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22683e91-642a-494b-abc5-069dc6fa9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics\n",
    "import numpy as np\n",
    "epsilon = np.finfo(float).eps\n",
    "\n",
    "def Wape(y, y_pred):\n",
    "    \"\"\"Weighted Average Percentage Error metric in the interval [0; 100]\"\"\"\n",
    "    y = np.array(y)\n",
    "    y_pred = np.array(y_pred)\n",
    "    nominator = np.sum(np.abs(np.subtract(y, y_pred)))\n",
    "    denominator = np.add(np.sum(np.abs(y)), epsilon)\n",
    "    wape = np.divide(nominator, denominator) * 100.0\n",
    "    return wape\n",
    "\n",
    "def nse(y, y_pred):\n",
    "    y = np.array(y)\n",
    "    y_pred = np.array(y_pred)\n",
    "    return (1-(np.sum((y_pred-y)**2)/np.sum((y-np.mean(y))**2)))\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    # following line prepares the model for evaulation mode. It disables dropout and batch normalization if they have \n",
    "    # are part of the model. For our simple model it's not necessary. Still I'm going to use it.\n",
    "\n",
    "    model.eval()\n",
    "    all_inputs = torch.empty((0, num_features, seq_len))\n",
    "    all_labels = torch.empty(0, pred_len)\n",
    "    for inputs, labels in data_loader:\n",
    "        all_inputs = torch.vstack((all_inputs, inputs))\n",
    "        all_labels = torch.vstack((all_labels, labels))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        all_inputs = all_inputs.to(device)\n",
    "        outputs = model(all_inputs).detach().cpu()\n",
    "        avg_val_loss = loss(outputs, all_labels)\n",
    "        nsee = nse(all_labels.numpy(), outputs.numpy())\n",
    "        wapee = Wape(all_labels.numpy(), outputs.numpy())\n",
    "        \n",
    "    print(f'NSE : {nsee}', end=' ')\n",
    "    print(f'WAPE : {wapee}', end=' ')\n",
    "    print(f'Validation Loss: {avg_val_loss}')\n",
    "    model.train()\n",
    "    return avg_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9beaf3a1-2c40-4fc6-b0f7-c352b5763231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 16:53:07,970] A new study created in memory with name: no-name-8950f954-95a8-4f38-b603-574277c5572a\n",
      "/var/folders/j3/9_pszf256zvgnry9z0x72k_h0000gn/T/ipykernel_29093/4271682891.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('lr', 1e-4, 1e-1)\n",
      "/var/folders/j3/9_pszf256zvgnry9z0x72k_h0000gn/T/ipykernel_29093/4271682891.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n",
      "/var/folders/j3/9_pszf256zvgnry9z0x72k_h0000gn/T/ipykernel_29093/4271682891.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_p = trial.suggest_uniform('dropout_p', 0.0, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Traning Loss: 0.02138136008195579 NSE : -0.004740238189697266 WAPE : 49.405791019343084 Validation Loss: 0.011892914772033691\n",
      "Epoch 2: Traning Loss: 0.018876492513343692 NSE : 0.012080132961273193 WAPE : 47.82370479826412 Validation Loss: 0.01169381570070982\n",
      "Epoch 3: Traning Loss: 0.01840300301462412 NSE : -0.016654253005981445 WAPE : 46.842339085660775 Validation Loss: 0.012033938430249691\n",
      "Epoch 4: Traning Loss: 0.018120203169062734 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 16:53:48,192] Trial 0 finished with value: 0.01169381570070982 and parameters: {'lr': 0.00016194304342083342, 'weight_decay': 6.841424555602147e-05, 'dropout_p': 0.40027493799568015}. Best is trial 0 with value: 0.01169381570070982.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.011871337890625 WAPE : 46.606716848438644 Validation Loss: 0.01197732426226139\n",
      "Early stopping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j3/9_pszf256zvgnry9z0x72k_h0000gn/T/ipykernel_29093/4271682891.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('lr', 1e-4, 1e-1)\n",
      "/var/folders/j3/9_pszf256zvgnry9z0x72k_h0000gn/T/ipykernel_29093/4271682891.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n",
      "/var/folders/j3/9_pszf256zvgnry9z0x72k_h0000gn/T/ipykernel_29093/4271682891.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_p = trial.suggest_uniform('dropout_p', 0.0, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Traning Loss: 0.01877187854703516 NSE : -0.009142398834228516 WAPE : 49.07704940717934 Validation Loss: 0.011945025064051151\n",
      "Epoch 2: Traning Loss: 0.01854699834343046 NSE : -0.008787989616394043 WAPE : 49.04997337290652 Validation Loss: 0.01194082759320736\n",
      "Epoch 3: Traning Loss: 0.018543006581254302 NSE : -0.008099794387817383 WAPE : 48.99615329419173 Validation Loss: 0.011932680383324623\n",
      "Epoch 4: Traning Loss: 0.018541007621213794 NSE : -0.007363557815551758 WAPE : 48.93673088708135 Validation Loss: 0.011923967860639095\n",
      "Epoch 5: Traning Loss: 0.018537821112200616 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 16:54:38,595] Trial 1 finished with value: 0.011915133334696293 and parameters: {'lr': 0.0002988266883817752, 'weight_decay': 5.5655669735639195e-05, 'dropout_p': 0.381211877661159}. Best is trial 0 with value: 0.01169381570070982.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.006617188453674316 WAPE : 48.87427907276447 Validation Loss: 0.011915133334696293\n",
      "Epoch 1: Traning Loss: 0.019335698308888823 NSE : -0.03278648853302002 WAPE : 50.46701537671263 Validation Loss: 0.01222489308565855\n",
      "Epoch 2: Traning Loss: 0.018862698198761792 NSE : -0.039726853370666504 WAPE : 50.8017931676191 Validation Loss: 0.012307043187320232\n",
      "Epoch 3: Traning Loss: 0.018813551472034305 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 16:55:09,539] Trial 2 finished with value: 0.01222489308565855 and parameters: {'lr': 0.0007573436945323, 'weight_decay': 0.0011534209960972512, 'dropout_p': 0.32493002350507727}. Best is trial 0 with value: 0.01169381570070982.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.044745564460754395 WAPE : 51.03313349054561 Validation Loss: 0.01236644946038723\n",
      "Early stopping!\n",
      "Epoch 1: Traning Loss: 0.03006215603835881 NSE : 0.011563658714294434 WAPE : 47.97291159156076 Validation Loss: 0.011699928902089596\n",
      "Epoch 2: Traning Loss: 0.019834825143218042 NSE : 0.1303076148033142 WAPE : 45.07158832536633 Validation Loss: 0.01029437966644764\n",
      "Epoch 3: Traning Loss: 0.017032656334340573 NSE : 0.24961018562316895 WAPE : 41.55711002248628 Validation Loss: 0.008882218040525913\n",
      "Epoch 4: Traning Loss: 0.014663853550329804 NSE : 0.36523133516311646 WAPE : 37.88078290445529 Validation Loss: 0.007513634394854307\n",
      "Epoch 5: Traning Loss: 0.012781767981126905 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 16:56:00,335] Trial 3 finished with value: 0.006337152794003487 and parameters: {'lr': 0.00010062111955309728, 'weight_decay': 0.0018651433403429928, 'dropout_p': 0.30544139646709195}. Best is trial 3 with value: 0.006337152794003487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : 0.46462303400039673 WAPE : 34.25243586220551 Validation Loss: 0.006337152794003487\n",
      "Epoch 1: Traning Loss: 0.01916814944264479 NSE : -0.033257365226745605 WAPE : 50.490394251505535 Validation Loss: 0.012230467051267624\n",
      "Epoch 2: Traning Loss: 0.018855348623357714 NSE : -0.04117751121520996 WAPE : 50.869500230053866 Validation Loss: 0.012324215844273567\n",
      "Epoch 3: Traning Loss: 0.01878470928175375 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 16:56:30,749] Trial 4 finished with value: 0.012230467051267624 and parameters: {'lr': 0.0007647318895243781, 'weight_decay': 0.0011153948667736694, 'dropout_p': 0.14728290448677217}. Best is trial 3 with value: 0.006337152794003487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.044689297676086426 WAPE : 51.03057943241509 Validation Loss: 0.012365785427391529\n",
      "Early stopping!\n",
      "Epoch 1: Traning Loss: 0.019598420839291066 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 16:56:40,912] Trial 5 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.016392827033996582 WAPE : 49.56996753592168 Validation Loss: 0.012030845507979393\n",
      "Epoch 1: Traning Loss: 0.023479830966796726 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 16:56:51,148] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.25265049934387207 WAPE : 58.01619627480247 Validation Loss: 0.014827379956841469\n",
      "Epoch 1: Traning Loss: 0.018309891231358053 NSE : -0.0007570981979370117 WAPE : 47.57050596306553 Validation Loss: 0.011845767498016357\n",
      "Epoch 2: Traning Loss: 0.018452448347583413 NSE : -0.0014967918395996094 WAPE : 47.459315778012886 Validation Loss: 0.011854524724185467\n",
      "Epoch 3: Traning Loss: 0.018425532422959804 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 16:57:21,784] Trial 7 finished with value: 0.011845767498016357 and parameters: {'lr': 0.00023888792655853055, 'weight_decay': 0.0046018639029721635, 'dropout_p': 0.232572141827935}. Best is trial 3 with value: 0.006337152794003487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.002379179000854492 WAPE : 47.36429877937903 Validation Loss: 0.01186496764421463\n",
      "Early stopping!\n",
      "Epoch 1: Traning Loss: 0.022090568671002984 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 16:57:31,969] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.00888669490814209 WAPE : 49.078347185609026 Validation Loss: 0.011941997334361076\n",
      "Epoch 1: Traning Loss: 0.040203986243577676 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 16:57:42,228] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.024647116661071777 WAPE : 50.04466772269613 Validation Loss: 0.012128547765314579\n",
      "Epoch 1: Traning Loss: 0.08001132544735447 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 16:57:52,447] Trial 10 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.022893905639648438 WAPE : 49.94850385010116 Validation Loss: 0.012107795104384422\n",
      "Epoch 1: Traning Loss: 0.03482726379856467 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 16:58:02,940] Trial 11 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.030905604362487793 WAPE : 46.61697080707784 Validation Loss: 0.012202630750834942\n",
      "Epoch 1: Traning Loss: 0.024911257274448873 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 16:58:13,727] Trial 12 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.18161511421203613 WAPE : 47.54232455356045 Validation Loss: 0.013986548408865929\n",
      "Epoch 1: Traning Loss: 0.022149462811183185 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 16:58:24,628] Trial 13 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.18958961963653564 WAPE : 56.172294573369676 Validation Loss: 0.01408094260841608\n",
      "Epoch 1: Traning Loss: 0.019665456728544087 NSE : 0.26971375942230225 WAPE : 41.95841404836096 Validation Loss: 0.008644256740808487\n",
      "Epoch 2: Traning Loss: 0.012408047121018171 NSE : 0.5593275129795074 WAPE : 30.41528985202157 Validation Loss: 0.0052161552011966705\n",
      "Epoch 3: Traning Loss: 0.008443294410593808 NSE : 0.7279270887374878 WAPE : 24.767520348335804 Validation Loss: 0.003220474347472191\n",
      "Epoch 4: Traning Loss: 0.0068508101562038065 NSE : 0.7711854428052902 WAPE : 22.865214844955467 Validation Loss: 0.0027084338944405317\n",
      "Epoch 5: Traning Loss: 0.006477498605847358 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 16:59:17,522] Trial 14 finished with value: 0.0024257085751742125 and parameters: {'lr': 0.00029409586609282086, 'weight_decay': 0.0005606064055967145, 'dropout_p': 0.18936945301450742}. Best is trial 14 with value: 0.0024257085751742125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : 0.7950707077980042 WAPE : 21.549820204870926 Validation Loss: 0.0024257085751742125\n",
      "Epoch 1: Traning Loss: 0.022872959461528806 NSE : 0.08119946718215942 WAPE : 45.287276082290624 Validation Loss: 0.010875665582716465\n",
      "Epoch 2: Traning Loss: 0.01680846110265702 NSE : 0.2730065584182739 WAPE : 40.79196400505892 Validation Loss: 0.008605279959738255\n",
      "Epoch 3: Traning Loss: 0.01248659360781312 NSE : 0.54670250415802 WAPE : 31.47593322718713 Validation Loss: 0.00536559522151947\n",
      "Epoch 4: Traning Loss: 0.008650019470602274 NSE : 0.6838905811309814 WAPE : 26.94885953193357 Validation Loss: 0.0037417258135974407\n",
      "Epoch 5: Traning Loss: 0.007276059971190989 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 17:00:11,951] Trial 15 finished with value: 0.00324849970638752 and parameters: {'lr': 0.0003635986405603939, 'weight_decay': 0.000724155154832045, 'dropout_p': 0.180785869026715}. Best is trial 14 with value: 0.0024257085751742125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : 0.7255594432353973 WAPE : 25.030316707735317 Validation Loss: 0.00324849970638752\n",
      "Epoch 1: Traning Loss: 0.02264887248980813 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 17:00:22,522] Trial 16 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.11314988136291504 WAPE : 53.704949923106625 Validation Loss: 0.013176139444112778\n",
      "Epoch 1: Traning Loss: 0.020169242594391106 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 17:00:33,372] Trial 17 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.03541207313537598 WAPE : 50.59593683682202 Validation Loss: 0.012255973182618618\n",
      "Epoch 1: Traning Loss: 0.02353827174473554 NSE : 0.26146310567855835 WAPE : 39.0458974432105 Validation Loss: 0.00874191801995039\n",
      "Epoch 2: Traning Loss: 0.008782417987007648 NSE : 0.5659763514995575 WAPE : 28.75150017904815 Validation Loss: 0.005137453321367502\n",
      "Epoch 3: Traning Loss: 0.009085985296405853 NSE : 0.11166989803314209 WAPE : 49.086499799523416 Validation Loss: 0.01051499042659998\n",
      "Epoch 4: Traning Loss: 0.0080467936352361 NSE : 0.7697699069976807 WAPE : 21.860551368711075 Validation Loss: 0.0027251888532191515\n",
      "Epoch 5: Traning Loss: 0.007442744900006801 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 17:01:27,859] Trial 18 finished with value: 0.0027251888532191515 and parameters: {'lr': 0.002244854964181319, 'weight_decay': 0.00015666065923624156, 'dropout_p': 0.16416494504439288}. Best is trial 14 with value: 0.0024257085751742125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : 0.7243680059909821 WAPE : 24.569701566795825 Validation Loss: 0.003262602724134922\n",
      "Epoch 1: Traning Loss: 0.02056050811242312 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 17:01:39,100] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.15606296062469482 WAPE : 55.13053311983369 Validation Loss: 0.01368409302085638\n",
      "Number of finished trials: 20\n",
      "Best trial:\n",
      "  Value (Best Validation Loss): 0.0024257085751742125\n",
      "  Params:\n",
      "    lr: 0.00029409586609282086\n",
      "    weight_decay: 0.0005606064055967145\n",
      "    dropout_p: 0.18936945301450742\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Here we define the search space of the hyper-parameters. Optuna uses byaesian optimization to find the optimal values of the hyperparameters.\n",
    "    learning_rate = trial.suggest_loguniform('lr', 1e-4, 1e-1)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n",
    "    dropout_p = trial.suggest_uniform('dropout_p', 0.0, 0.5)\n",
    "    \n",
    "    model = BasicMLPNetwork(seq_len, pred_len, num_features, dropout_p)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    num_epochs = 5\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 2\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = []\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss_val = loss(outputs, labels)\n",
    "    \n",
    "            # calculate gradients for back propagation\n",
    "            loss_val.backward()\n",
    "    \n",
    "            # update the weights based on the gradients\n",
    "            optimizer.step()\n",
    "    \n",
    "            # reset the gradients, avoid gradient accumulation\n",
    "            optimizer.zero_grad()\n",
    "            epoch_loss.append(loss_val.item())\n",
    "    \n",
    "        avg_train_loss = sum(epoch_loss)/len(epoch_loss)\n",
    "        print(f'Epoch {epoch+1}: Traning Loss: {avg_train_loss}', end=' ')\n",
    "        avg_val_loss = evaluate_model(model, val_loader)\n",
    "    \n",
    "        # Check for improvement\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), 'best_model_trial.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print('Early stopping!')\n",
    "                # Load the best model before stopping\n",
    "                model.load_state_dict(torch.load('best_model_trial.pth'))\n",
    "                break\n",
    "\n",
    "        # Report intermediate objective value\n",
    "        trial.report(best_val_loss, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "print('  Value (Best Validation Loss):', trial.value)\n",
    "print('  Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79e3db5c-1a0a-4b66-ad8e-cf4686fa94ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/cybertraining/venv/lib/python3.12/site-packages/optuna/visualization/_plotly_imports.py:7\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m try_import() \u001b[38;5;28;01mas\u001b[39;00m _imports:\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m plotly_version\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisualization\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mvis\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Optimization history\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_optimization_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Hyperparameter importance\u001b[39;00m\n\u001b[1;32m      7\u001b[0m vis\u001b[38;5;241m.\u001b[39mplot_param_importances(study)\n",
      "File \u001b[0;32m~/Documents/cybertraining/venv/lib/python3.12/site-packages/optuna/visualization/_optimization_history.py:200\u001b[0m, in \u001b[0;36mplot_optimization_history\u001b[0;34m(study, target, target_name, error_bar)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_optimization_history\u001b[39m(\n\u001b[1;32m    173\u001b[0m     study: Study \u001b[38;5;241m|\u001b[39m Sequence[Study],\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     error_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    178\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgo.Figure\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Plot optimization history of all trials in a study.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m        A :class:`plotly.graph_objects.Figure` object.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     \u001b[43m_imports\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     info_list \u001b[38;5;241m=\u001b[39m _get_optimization_history_info_list(study, target, target_name, error_bar)\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_optimization_history_plot(info_list, target_name)\n",
      "File \u001b[0;32m~/Documents/cybertraining/venv/lib/python3.12/site-packages/optuna/_imports.py:95\u001b[0m, in \u001b[0;36m_DeferredImportExceptionContextManager.check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deferred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m     exc_value, message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deferred\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc_value\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'."
     ]
    }
   ],
   "source": [
    "import optuna.visualization as vis\n",
    "\n",
    "# Optimization history\n",
    "vis.plot_optimization_history(study)\n",
    "\n",
    "# Hyperparameter importance\n",
    "vis.plot_param_importances(study)\n",
    "\n",
    "## We need to make sure we can to plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7216e93-a35d-4aa5-83db-2e43ea380f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adcb080-98f7-4dfa-bc75-298d94becc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa21e49-a774-4a21-ba77-aad2d3617f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6736f873-ed65-4d8c-942e-fc227c1cae0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
