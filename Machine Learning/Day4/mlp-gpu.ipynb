{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ccae883-1904-48a6-81b3-d0228caeab1f",
   "metadata": {},
   "source": [
    "### Multivariate time series prediction using MLP\n",
    "\n",
    "\n",
    "Here is a pytorch implementation of a fully connected neural network which has in total 4 fully connected layers. This is a vanilla\n",
    "neural network model to be used for multi variate time series prediction. The data we have is 2D, the model learns first\n",
    "along the time axis and secondly it learns along the feature axis. It has two fully connected layers with size\n",
    "**seq_len x 100** and **100 x pred_len** with **ReLU** loss function along time axis and two fully connected layers with sizes **num_features x 20** and **20 x 1** with same **ReLU** as loss function along the features axis. The neural network uses Mean Square Error Loss (MSELoss) to calculate the model loss. NSE and WAPE are used for model evaluation. The data is normalized using StandardScaler from scikit learn. Data is used is the 3 hr river gauge height data. It is divided in the ratio of 7:1:2 training, validation and testing respectively.\n",
    "\n",
    "This notebooks implements MLP with **regularization and early stopping** with model implemented to fully use **GPU**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4117e9d0-7925-4788-bbe1-28c60c80bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8af4f85b-3822-4288-9566-0f0ac0cfa053",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'chattahoochee_3hr_02336490.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc9eb646-4308-449c-84c0-00cd1de182c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Implement determinism. Set a fixed value for random seed so that when the parameters are initialized, they are initialized same across all experiments.\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# If you are using CUDA, also set the seed for it\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Set the seed for NumPy\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0600e784-ec1c-4ab8-ab6b-9b765b4dee83",
   "metadata": {},
   "source": [
    "Here we define **RiverData** a custom Dataset class to load the dataset we have. It extends the pytorch Dataset class.  \n",
    "- We need to define \\_\\_init__() function which can be used for loading data from file and optionally for data preprocessing.\n",
    "- Thereafter we define \\_\\_len__() function which gives the length of dataset.\n",
    "- Then we define \\_\\_getitem__() function which returns an instance of (feature, label) tuple which can be used for model training.\n",
    "  For our time series data, feature means the past values to be used for training and label means the future values to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "742b694d-9966-4fd3-bca7-51f398a8775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiverData(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, df, target, datecol, seq_len, pred_len):\n",
    "        self.df = df\n",
    "        self.datecol = datecol\n",
    "        self.target = target\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.setIndex()\n",
    "        \n",
    "\n",
    "    def setIndex(self):\n",
    "        self.df.set_index(self.datecol, inplace=True)\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) - self.seq_len - self.pred_len\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if len(self.df) <= (idx + self.seq_len+self.pred_len):\n",
    "            raise IndexError(f\"Index {idx} is out of bounds for dataset of size {len(self.df)}\")\n",
    "        df_piece = self.df[idx:idx+self.seq_len].values\n",
    "        feature = torch.tensor(df_piece, dtype=torch.float32)\n",
    "        label_piece = self.df[self.target][idx + self.seq_len:  idx+self.seq_len+self.pred_len].values\n",
    "        label = torch.tensor(label_piece, dtype=torch.float32)\n",
    "        return (feature.T, label) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c24959-e72f-435d-8a14-69a648fa5ab1",
   "metadata": {},
   "source": [
    "### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d897d9-946e-4bbe-90a6-cc8d8a223c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)\n",
    "raw_df = df.drop('DATE', axis=1, inplace=False)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply the transformations\n",
    "df_scaled = scaler.fit_transform(raw_df)\n",
    "\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=raw_df.columns)\n",
    "df_scaled['DATE'] = df['DATE']\n",
    "df = df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4781c6-88ed-4b28-aa0e-f669bf7c75f5",
   "metadata": {},
   "source": [
    "Some advanced python syntax have been used here. \\\n",
    "*common_args : it's used to pass arguments to a function, where common_args represents a python list \\\n",
    "**common_args: it's used to pass arguments to a function, where common_args represents a python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e63a535-a817-4429-be17-f3e3a02e4f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_size = int(0.7 * len(df))\n",
    "test_size = int(0.2 * len(df))\n",
    "val_size = len(df) - train_size - test_size\n",
    "\n",
    "seq_len = 8\n",
    "pred_len = 1\n",
    "num_features = 7\n",
    "\n",
    "common_args = ['gaze_height', 'DATE', seq_len, pred_len]\n",
    "train_dataset = RiverData(df[:train_size], *common_args)\n",
    "val_dataset = RiverData(df[train_size: train_size+val_size], *common_args)\n",
    "test_dataset = RiverData(df[train_size+val_size : len(df)], *common_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "501879b1-dd1c-4ff0-afb3-ff25a345df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important parameters\n",
    "\n",
    "BATCH_SIZE = 512 # keep as big as can be handled by GPU and memory\n",
    "SHUFFLE = False # we don't shuffle the time series data\n",
    "DATA_LOAD_WORKERS = 1 # it depends on amount of data you need to load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae359aec-9435-4229-8587-5f120b0370b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "common_args = {'batch_size': BATCH_SIZE, 'shuffle': SHUFFLE}\n",
    "train_loader = DataLoader(train_dataset, **common_args)\n",
    "val_loader = DataLoader(val_dataset, **common_args)\n",
    "test_loader = DataLoader(test_dataset, **common_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ac35c-8ecc-4ab7-a97c-0cb4d59bc624",
   "metadata": {},
   "source": [
    "### Here we define our pytorch model.\n",
    "\n",
    "BasicMLPNetwork is the model class, it extends the Module class provided by pytorch. \\\n",
    "- We define \\_\\_init__() function. It sets up layers and defines the model parameters.\n",
    "- Also, we define forward() function which defines how the forwared pass computation occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9b72d79-65b6-4fd0-9d0b-025c2a0fc293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are adding dropout layers.\n",
    "\n",
    "class BasicMLPNetwork(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len, pred_len, num_features, dropout):\n",
    "        # call the constructor of the base class\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.num_features = num_features\n",
    "        hidden_size_time = 512\n",
    "        hidden_size_feat = 128\n",
    "        # define layers for combining across time series\n",
    "        self.fc1 = torch.nn.Linear(self.seq_len, hidden_size_time)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout1 = torch.nn.Dropout(p=dropout)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size_time, self.pred_len)\n",
    "        self.dropout2 = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        # define layers for combining across the features\n",
    "        self.fc3 = torch.nn.Linear(self.num_features, hidden_size_feat)\n",
    "        self.fc4 = torch.nn.Linear(hidden_size_feat, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # computation over time\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out) # has dimension 512 x 7 x 12\n",
    "        out = self.dropout2(out)\n",
    "        # computation over features\n",
    "        out = out.transpose(1,2) # dimension 512 x 12 x 7\n",
    "        out = self.fc3(out) # dimension 512 x 12 x 20\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out) # dimension 512 x 12 x 1\n",
    "\n",
    "        out = out.squeeze(-1) # dimension 512 x 12\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Note that the gradients are stored insize the FC layer objects\n",
    "# For each training example we need to get rid of these gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5277794b-fe37-4595-8554-d26db5710e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_p = 0.03503\n",
    "learning_rate = 0.00804\n",
    "weight_decay = 0.000201  # Regularization strength\n",
    "\n",
    "model = BasicMLPNetwork(seq_len, pred_len, num_features, dropout_p)\n",
    "model = model.to(device)\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1afde643-9953-4129-8415-7b4836c31300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "torch.Size([512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1])\n",
      "torch.Size([128, 7])\n",
      "torch.Size([128])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for gen in model.parameters():\n",
    "    print(gen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b3cc7a8-9844-4eae-a601-4787513eb1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape:  torch.Size([512, 7, 8])\n",
      "labels shape:  torch.Size([512, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, (f,l) in enumerate(train_loader):\n",
    "    print('features shape: ', f.shape)\n",
    "    print('labels shape: ', l.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22683e91-642a-494b-abc5-069dc6fa9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics\n",
    "import numpy as np\n",
    "epsilon = np.finfo(float).eps\n",
    "\n",
    "def Wape(y, y_pred):\n",
    "    \"\"\"Weighted Average Percentage Error metric in the interval [0; 100]\"\"\"\n",
    "    y = np.array(y)\n",
    "    y_pred = np.array(y_pred)\n",
    "    nominator = np.sum(np.abs(np.subtract(y, y_pred)))\n",
    "    denominator = np.add(np.sum(np.abs(y)), epsilon)\n",
    "    wape = np.divide(nominator, denominator) * 100.0\n",
    "    return wape\n",
    "\n",
    "def nse(y, y_pred):\n",
    "    y = np.array(y)\n",
    "    y_pred = np.array(y_pred)\n",
    "    return (1-(np.sum((y_pred-y)**2)/np.sum((y-np.mean(y))**2)))\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    # following line prepares the model for evaulation mode. It disables dropout and batch normalization if they have \n",
    "    # are part of the model. For our simple model it's not necessary. Still I'm going to use it.\n",
    "\n",
    "    model.eval()\n",
    "    all_inputs = torch.empty((0, num_features, seq_len))\n",
    "    all_labels = torch.empty(0, pred_len)\n",
    "    for inputs, labels in data_loader:\n",
    "        all_inputs = torch.vstack((all_inputs, inputs))\n",
    "        all_labels = torch.vstack((all_labels, labels))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        all_inputs = all_inputs.to(device)\n",
    "        outputs = model(all_inputs).detach().cpu()\n",
    "        avg_val_loss = loss(outputs, all_labels)\n",
    "        nsee = nse(all_labels.numpy(), outputs.numpy())\n",
    "        wapee = Wape(all_labels.numpy(), outputs.numpy())\n",
    "        \n",
    "    print(f'NSE : {nsee}', end=' ')\n",
    "    print(f'WAPE : {wapee}', end=' ')\n",
    "    print(f'Validation Loss: {avg_val_loss}')\n",
    "    model.train()\n",
    "    return avg_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9beaf3a1-2c40-4fc6-b0f7-c352b5763231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Traning Loss: 0.7573124057260053 NSE : 0.6801067590713501 WAPE : 53.64146653171908 Validation Loss: 0.38514766097068787\n",
      "Epoch 2: Traning Loss: 0.2945940637126051 NSE : 0.7953104972839355 WAPE : 42.95421796988184 Validation Loss: 0.24644368886947632\n",
      "Epoch 3: Traning Loss: 0.21619502750450167 NSE : 0.8499813079833984 WAPE : 36.47109695305662 Validation Loss: 0.1806207299232483\n",
      "Epoch 4: Traning Loss: 0.18542799180180863 NSE : 0.8807359933853149 WAPE : 32.28761560534481 Validation Loss: 0.14359241724014282\n",
      "Epoch 5: Traning Loss: 0.16638764925301075 NSE : 0.8932132124900818 WAPE : 30.30788772066112 Validation Loss: 0.12857002019882202\n",
      "Epoch 6: Traning Loss: 0.15363641346579995 NSE : 0.9056360721588135 WAPE : 28.337854033731247 Validation Loss: 0.11361301690340042\n",
      "Epoch 7: Traning Loss: 0.13875344080914712 NSE : 0.9065907001495361 WAPE : 28.04559433741543 Validation Loss: 0.11246372014284134\n",
      "Epoch 8: Traning Loss: 0.13414746731648158 NSE : 0.9074494242668152 WAPE : 27.685876058948544 Validation Loss: 0.11142979562282562\n",
      "Epoch 9: Traning Loss: 0.12574673790870042 NSE : 0.9182255268096924 WAPE : 25.934985007393657 Validation Loss: 0.09845548123121262\n",
      "Epoch 10: Traning Loss: 0.13252565367468472 NSE : 0.9205446243286133 WAPE : 25.407474014662192 Validation Loss: 0.09566335380077362\n",
      "Epoch 11: Traning Loss: 0.12154397702422635 NSE : 0.923458993434906 WAPE : 24.908604081143203 Validation Loss: 0.09215447306632996\n",
      "Epoch 12: Traning Loss: 0.10841759111218412 NSE : 0.9213827252388 WAPE : 25.24971275847664 Validation Loss: 0.09465426206588745\n",
      "Epoch 13: Traning Loss: 0.12019824702292681 NSE : 0.9238200783729553 WAPE : 24.744623874902814 Validation Loss: 0.09171972423791885\n",
      "Epoch 14: Traning Loss: 0.12389974691102217 NSE : 0.9264605045318604 WAPE : 24.28760680724937 Validation Loss: 0.08854065835475922\n",
      "Epoch 15: Traning Loss: 0.11397320035331208 NSE : 0.9262757897377014 WAPE : 24.192465180722657 Validation Loss: 0.08876309543848038\n",
      "Epoch 16: Traning Loss: 0.11534944388630061 NSE : 0.9242348670959473 WAPE : 24.574688661470795 Validation Loss: 0.09122033417224884\n",
      "Epoch 17: Traning Loss: 0.11006883056513195 NSE : 0.9294067025184631 WAPE : 23.703076009679208 Validation Loss: 0.08499348908662796\n",
      "Epoch 18: Traning Loss: 0.11458121731491952 NSE : 0.9294706583023071 WAPE : 23.601329294464435 Validation Loss: 0.08491648733615875\n",
      "Epoch 19: Traning Loss: 0.10968838619260952 NSE : 0.9293411374092102 WAPE : 23.654463273816933 Validation Loss: 0.08507240563631058\n",
      "Epoch 20: Traning Loss: 0.11652263478729231 NSE : 0.9303573369979858 WAPE : 23.36080077006145 Validation Loss: 0.08384891599416733\n",
      "Epoch 21: Traning Loss: 0.10393938338705178 NSE : 0.9322208762168884 WAPE : 23.183040139518244 Validation Loss: 0.08160523325204849\n",
      "Epoch 22: Traning Loss: 0.10543560416534029 NSE : 0.930896520614624 WAPE : 23.354011573079912 Validation Loss: 0.08319977670907974\n",
      "Epoch 23: Traning Loss: 0.11146761181539502 NSE : 0.9342165589332581 WAPE : 22.826824806458195 Validation Loss: 0.07920249551534653\n",
      "Epoch 24: Traning Loss: 0.11042708148858671 NSE : 0.9315891265869141 WAPE : 23.193076485428072 Validation Loss: 0.08236591517925262\n",
      "Epoch 25: Traning Loss: 0.10654476410227604 NSE : 0.9343868494033813 WAPE : 22.72186352785379 Validation Loss: 0.07899747788906097\n",
      "Epoch 26: Traning Loss: 0.10737003715195019 NSE : 0.9350842833518982 WAPE : 22.587423741693133 Validation Loss: 0.07815776765346527\n",
      "Epoch 27: Traning Loss: 0.10692456293979595 NSE : 0.9330928921699524 WAPE : 22.941325501507734 Validation Loss: 0.08055536448955536\n",
      "Epoch 28: Traning Loss: 0.09961063067974715 NSE : 0.9382181763648987 WAPE : 21.996227572188374 Validation Loss: 0.07438454777002335\n",
      "Epoch 29: Traning Loss: 0.10446439192084403 NSE : 0.9328953623771667 WAPE : 23.036986867376203 Validation Loss: 0.08079318702220917\n",
      "Epoch 30: Traning Loss: 0.0961327738096488 NSE : 0.9383641481399536 WAPE : 22.007816619015745 Validation Loss: 0.0742088034749031\n",
      "Epoch 31: Traning Loss: 0.11015707106682761 NSE : 0.9349323511123657 WAPE : 22.568812511282744 Validation Loss: 0.07834070175886154\n",
      "Epoch 32: Traning Loss: 0.10518913590830975 NSE : 0.9328621625900269 WAPE : 22.972765658491824 Validation Loss: 0.08083315938711166\n",
      "Epoch 33: Traning Loss: 0.11439128615090559 NSE : 0.9406350255012512 WAPE : 21.46765592017325 Validation Loss: 0.07147472351789474\n",
      "Epoch 34: Traning Loss: 0.10059954690072558 NSE : 0.9341170787811279 WAPE : 22.75713900273914 Validation Loss: 0.07932227104902267\n",
      "Epoch 35: Traning Loss: 0.10335702082977213 NSE : 0.9354880452156067 WAPE : 22.61809813851852 Validation Loss: 0.07767161726951599\n",
      "Epoch 36: Traning Loss: 0.1038466405508847 NSE : 0.935156524181366 WAPE : 22.62542825544195 Validation Loss: 0.07807079702615738\n",
      "Epoch 37: Traning Loss: 0.09447756610361152 NSE : 0.9378260374069214 WAPE : 22.126382360014155 Validation Loss: 0.0748567208647728\n",
      "Epoch 38: Traning Loss: 0.10257488579071801 NSE : 0.9369453191757202 WAPE : 22.219097993187013 Validation Loss: 0.07591705769300461\n",
      "Early stopping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/slurm.769510/ipykernel_2590162/209109333.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = []\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss_val = loss(outputs, labels)\n",
    "\n",
    "        # calculate gradients for back propagation\n",
    "        loss_val.backward()\n",
    "\n",
    "        # update the weights based on the gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # reset the gradients, avoid gradient accumulation\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss.append(loss_val.item())\n",
    "\n",
    "    avg_train_loss = sum(epoch_loss)/len(epoch_loss)\n",
    "    print(f'Epoch {epoch+1}: Traning Loss: {avg_train_loss}', end=' ')\n",
    "    avg_val_loss = evaluate_model(model, val_loader)\n",
    "\n",
    "    # Check for improvement\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == patience:\n",
    "            print('Early stopping!')\n",
    "            # Load the best model before stopping\n",
    "            model.load_state_dict(torch.load('best_model.pth'))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7216e93-a35d-4aa5-83db-2e43ea380f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : 0.9203775525093079 WAPE : 25.542716825315097 Validation Loss: 0.05908162146806717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0591)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adcb080-98f7-4dfa-bc75-298d94becc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa21e49-a774-4a21-ba77-aad2d3617f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6736f873-ed65-4d8c-942e-fc227c1cae0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
