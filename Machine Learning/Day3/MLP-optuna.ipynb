{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ccae883-1904-48a6-81b3-d0228caeab1f",
   "metadata": {},
   "source": [
    "### Multivariate time series prediction using MLP with Hyperparameter optimization\n",
    "\n",
    "In this notebook, we use **Optuna** to find the optimum values of hyperparameters. Optuna is a package for optimizing hyperparameters. In this notebook we specifically optimize the values of **learning rate, weight decay and dropout**.\n",
    "\n",
    "Optuna is a python package specifially designed for hyperparameter tuning. We need to define a range of possible values for each of the hyperparameters. And optuna will try different parameter values with the model to minimize the validation loss after for specified number of experiments. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4117e9d0-7925-4788-bbe1-28c60c80bb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kpanthi/.conda/envs/cenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8af4f85b-3822-4288-9566-0f0ac0cfa053",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../dataset/final_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc9eb646-4308-449c-84c0-00cd1de182c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Implement determinism. Set a fixed value for random seed so that when the parameters are initialized, they are initialized same across all experiments.\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# If you are using CUDA, also set the seed for it\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Set the seed for NumPy\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0600e784-ec1c-4ab8-ab6b-9b765b4dee83",
   "metadata": {},
   "source": [
    "Here we define **RiverData** a custom Dataset class to load the dataset we have. It extends the pytorch Dataset class.  \n",
    "- We need to define \\_\\_init__() function which can be used for loading data from file and optionally for data preprocessing.\n",
    "- Thereafter we define \\_\\_len__() function which gives the length of dataset.\n",
    "- Then we define \\_\\_getitem__() function which returns an instance of (feature, label) tuple which can be used for model training.\n",
    "  For our time series data, feature means the past values to be used for training and label means the future values to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "742b694d-9966-4fd3-bca7-51f398a8775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiverData(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, df, target, datecol, seq_len, pred_len):\n",
    "        self.df = df\n",
    "        self.datecol = datecol\n",
    "        self.target = target\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.setIndex()\n",
    "        \n",
    "\n",
    "    def setIndex(self):\n",
    "        self.df.set_index(self.datecol, inplace=True)\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) - self.seq_len - self.pred_len\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if len(self.df) <= (idx + self.seq_len+self.pred_len):\n",
    "            raise IndexError(f\"Index {idx} is out of bounds for dataset of size {len(self.df)}\")\n",
    "        df_piece = self.df[idx:idx+self.seq_len].values\n",
    "        feature = torch.tensor(df_piece, dtype=torch.float32)\n",
    "        label_piece = self.df[self.target][idx + self.seq_len:  idx+self.seq_len+self.pred_len].values\n",
    "        label = torch.tensor(label_piece, dtype=torch.float32)\n",
    "        return (feature.T, label) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c24959-e72f-435d-8a14-69a648fa5ab1",
   "metadata": {},
   "source": [
    "### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d897d9-946e-4bbe-90a6-cc8d8a223c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)\n",
    "df = df[df['DATE'] > '2012']\n",
    "raw_df = df.drop('DATE', axis=1, inplace=False)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply the transformations\n",
    "df_scaled = scaler.fit_transform(raw_df)\n",
    "\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=raw_df.columns)\n",
    "df_scaled['DATE'] = df['DATE']\n",
    "df = df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4781c6-88ed-4b28-aa0e-f669bf7c75f5",
   "metadata": {},
   "source": [
    "Some advanced python syntax have been used here. \\\n",
    "*common_args : it's used to pass arguments to a function, where common_args represents a python list \\\n",
    "**common_args: it's used to pass arguments to a function, where common_args represents a python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e63a535-a817-4429-be17-f3e3a02e4f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_size = int(0.7 * len(df))\n",
    "test_size = int(0.2 * len(df))\n",
    "val_size = len(df) - train_size - test_size\n",
    "\n",
    "seq_len = 13\n",
    "pred_len = 1\n",
    "num_features = 7\n",
    "\n",
    "common_args = ['gauge_height', 'DATE', seq_len, pred_len]\n",
    "train_dataset = RiverData(df[:train_size], *common_args)\n",
    "val_dataset = RiverData(df[train_size: train_size+val_size], *common_args)\n",
    "test_dataset = RiverData(df[train_size+val_size : len(df)], *common_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "501879b1-dd1c-4ff0-afb3-ff25a345df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important parameters\n",
    "\n",
    "BATCH_SIZE = 512 # keep as big as can be handled by GPU and memory\n",
    "SHUFFLE = False # we don't shuffle the time series data\n",
    "DATA_LOAD_WORKERS = 1 # it depends on amount of data you need to load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae359aec-9435-4229-8587-5f120b0370b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "common_args = {'batch_size': BATCH_SIZE, 'shuffle': SHUFFLE}\n",
    "train_loader = DataLoader(train_dataset, **common_args)\n",
    "val_loader = DataLoader(val_dataset, **common_args)\n",
    "test_loader = DataLoader(test_dataset, **common_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ac35c-8ecc-4ab7-a97c-0cb4d59bc624",
   "metadata": {},
   "source": [
    "### Here we define our pytorch model.\n",
    "\n",
    "BasicMLPNetwork is the model class, it extends the Module class provided by pytorch. \\\n",
    "- We define \\_\\_init__() function. It sets up layers and defines the model parameters.\n",
    "- Also, we define forward() function which defines how the forwared pass computation occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9b72d79-65b6-4fd0-9d0b-025c2a0fc293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are adding dropout layers.\n",
    "\n",
    "class BasicMLPNetwork(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len, pred_len, num_features, dropout):\n",
    "        # call the constructor of the base class\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.num_features = num_features\n",
    "        hidden_size_time = 256\n",
    "        hidden_size_feat = 128\n",
    "        # define layers for combining across time series\n",
    "        self.fc1 = torch.nn.Linear(self.seq_len, hidden_size_time)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout1 = torch.nn.Dropout(p=dropout)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size_time, self.pred_len)\n",
    "        self.dropout2 = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        # define layers for combining across the features\n",
    "        self.fc3 = torch.nn.Linear(self.num_features, hidden_size_feat)\n",
    "        self.fc4 = torch.nn.Linear(hidden_size_feat, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # computation over time\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out) # has dimension 512 x 7 x 12\n",
    "        out = self.dropout2(out)\n",
    "        # computation over features\n",
    "        out = out.transpose(1,2) # dimension 512 x 12 x 7\n",
    "        out = self.fc3(out) # dimension 512 x 12 x 20\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out) # dimension 512 x 12 x 1\n",
    "\n",
    "        out = out.squeeze(-1) # dimension 512 x 12\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Note that the gradients are stored insize the FC layer objects\n",
    "# For each training example we need to get rid of these gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5277794b-fe37-4595-8554-d26db5710e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1afde643-9953-4129-8415-7b4836c31300",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b3cc7a8-9844-4eae-a601-4787513eb1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape:  torch.Size([512, 7, 13])\n",
      "labels shape:  torch.Size([512, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, (f,l) in enumerate(train_loader):\n",
    "    print('features shape: ', f.shape)\n",
    "    print('labels shape: ', l.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22683e91-642a-494b-abc5-069dc6fa9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics\n",
    "import numpy as np\n",
    "epsilon = np.finfo(float).eps\n",
    "\n",
    "def Wape(y, y_pred):\n",
    "    \"\"\"Weighted Average Percentage Error metric in the interval [0; 100]\"\"\"\n",
    "    y = np.array(y)\n",
    "    y_pred = np.array(y_pred)\n",
    "    nominator = np.sum(np.abs(np.subtract(y, y_pred)))\n",
    "    denominator = np.add(np.sum(np.abs(y)), epsilon)\n",
    "    wape = np.divide(nominator, denominator) * 100.0\n",
    "    return wape\n",
    "\n",
    "def nse(y, y_pred):\n",
    "    y = np.array(y)\n",
    "    y_pred = np.array(y_pred)\n",
    "    return (1-(np.sum((y_pred-y)**2)/np.sum((y-np.mean(y))**2)))\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    # following line prepares the model for evaulation mode. It disables dropout and batch normalization if they have \n",
    "    # are part of the model. For our simple model it's not necessary. Still I'm going to use it.\n",
    "\n",
    "    model.eval()\n",
    "    all_inputs = torch.empty((0, num_features, seq_len))\n",
    "    all_labels = torch.empty(0, pred_len)\n",
    "    for inputs, labels in data_loader:\n",
    "        all_inputs = torch.vstack((all_inputs, inputs))\n",
    "        all_labels = torch.vstack((all_labels, labels))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        all_inputs = all_inputs.to(device)\n",
    "        outputs = model(all_inputs).detach().cpu()\n",
    "        avg_val_loss = loss(outputs, all_labels)\n",
    "        nsee = nse(all_labels.numpy(), outputs.numpy())\n",
    "        wapee = Wape(all_labels.numpy(), outputs.numpy())\n",
    "        \n",
    "    print(f'NSE : {nsee}', end=' ')\n",
    "    print(f'WAPE : {wapee}', end=' ')\n",
    "    print(f'Validation Loss: {avg_val_loss}')\n",
    "    model.train()\n",
    "    return avg_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9beaf3a1-2c40-4fc6-b0f7-c352b5763231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:24:36,665] A new study created in memory with name: no-name-c27094dc-e043-46e1-8cf0-9e6ea119370a\n",
      "/local_scratch/slurm.1125506/ipykernel_3258377/3985622544.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('lr', 1e-4, 1e-1)\n",
      "/local_scratch/slurm.1125506/ipykernel_3258377/3985622544.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n",
      "/local_scratch/slurm.1125506/ipykernel_3258377/3985622544.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_p = trial.suggest_uniform('dropout_p', 0.0, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Traning Loss: 0.01916072037986686 NSE : -0.01572859287261963 WAPE : 43.30573329081579 Validation Loss: 0.013396549969911575\n",
      "Epoch 2: Traning Loss: 0.017773940703744107 NSE : -0.016893386840820312 WAPE : 43.37999627116872 Validation Loss: 0.013411909341812134\n",
      "Epoch 3: Traning Loss: 0.01730379935794654 NSE : 0.04941713809967041 WAPE : 41.90851795757358 Validation Loss: 0.012537333182990551\n",
      "Epoch 4: Traning Loss: 0.012876993881213486 NSE : 0.46912115812301636 WAPE : 26.004010730044726 Validation Loss: 0.0070018162950873375\n",
      "Epoch 5: Traning Loss: 0.010350052168867831 NSE : 0.5256786644458771 WAPE : 23.640776635479252 Validation Loss: 0.006255871616303921\n",
      "Epoch 6: Traning Loss: 0.010038072848067488 NSE : 0.5434865951538086 WAPE : 22.64961717295897 Validation Loss: 0.0060210018418729305\n",
      "Epoch 7: Traning Loss: 0.009996265862074574 NSE : 0.550794780254364 WAPE : 22.102456457603743 Validation Loss: 0.00592461321502924\n",
      "Epoch 8: Traning Loss: 0.00992068236900006 NSE : 0.5583174526691437 WAPE : 21.641280867724568 Validation Loss: 0.005825395230203867\n",
      "Epoch 9: Traning Loss: 0.00979610481580732 NSE : 0.5683338642120361 WAPE : 21.060175569260423 Validation Loss: 0.005693289451301098\n",
      "Epoch 10: Traning Loss: 0.009790746387306776 NSE : 0.570722907781601 WAPE : 21.026675544005172 Validation Loss: 0.005661779083311558\n",
      "Epoch 11: Traning Loss: 0.00965965844553289 NSE : 0.5767178535461426 WAPE : 20.642163001418982 Validation Loss: 0.005582711659371853\n",
      "Epoch 12: Traning Loss: 0.009641521046213155 NSE : 0.5809466242790222 WAPE : 20.445341148242207 Validation Loss: 0.005526938010007143\n",
      "Epoch 13: Traning Loss: 0.00973432760798191 NSE : 0.5786511301994324 WAPE : 20.452904086452158 Validation Loss: 0.005557213444262743\n",
      "Epoch 14: Traning Loss: 0.009694432011561706 NSE : 0.5840818285942078 WAPE : 20.236269407950395 Validation Loss: 0.005485587287694216\n",
      "Epoch 15: Traning Loss: 0.009724795729259416 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:26:11,321] Trial 0 finished with value: 0.005485587287694216 and parameters: {'lr': 0.0004182462427868436, 'weight_decay': 4.137577281552585e-05, 'dropout_p': 0.1917303324665699}. Best is trial 0 with value: 0.005485587287694216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : 0.5798713862895966 WAPE : 20.21062141987742 Validation Loss: 0.005541119258850813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/slurm.1125506/ipykernel_3258377/3985622544.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('lr', 1e-4, 1e-1)\n",
      "/local_scratch/slurm.1125506/ipykernel_3258377/3985622544.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n",
      "/local_scratch/slurm.1125506/ipykernel_3258377/3985622544.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_p = trial.suggest_uniform('dropout_p', 0.0, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Traning Loss: 1.9483975153900808 NSE : -1.075688362121582 WAPE : 59.20074934453626 Validation Loss: 0.027376465499401093\n",
      "Epoch 2: Traning Loss: 0.025183055634241575 NSE : -0.298305869102478 WAPE : 52.95254598555531 Validation Loss: 0.017123490571975708\n",
      "Epoch 3: Traning Loss: 0.020061713079656256 NSE : -0.03237128257751465 WAPE : 44.239370239889965 Validation Loss: 0.013616050593554974\n",
      "Epoch 4: Traning Loss: 0.018798368520825327 NSE : -0.011872172355651855 WAPE : 43.04839156786664 Validation Loss: 0.013345684856176376\n",
      "Epoch 5: Traning Loss: 0.019059648001902494 NSE : -0.00027191638946533203 WAPE : 41.82888482744228 Validation Loss: 0.013192687183618546\n",
      "Epoch 6: Traning Loss: 0.019261704693351492 NSE : -0.0056732892990112305 WAPE : 41.00424910032945 Validation Loss: 0.013263927772641182\n",
      "Epoch 7: Traning Loss: 0.01940465645982584 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:26:54,189] Trial 1 finished with value: 0.013192687183618546 and parameters: {'lr': 0.07743412458199854, 'weight_decay': 1.4788727760527778e-05, 'dropout_p': 0.13864611869658144}. Best is trial 0 with value: 0.005485587287694216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.02276754379272461 WAPE : 40.6339296808751 Validation Loss: 0.01348938513547182\n",
      "Early stopping!\n",
      "Epoch 1: Traning Loss: 0.02436667114211834 NSE : -0.0013695955276489258 WAPE : 41.300613729844756 Validation Loss: 0.013207166455686092\n",
      "Epoch 2: Traning Loss: 0.01986140956037253 NSE : -0.007388949394226074 WAPE : 42.70969303034743 Validation Loss: 0.01328655332326889\n",
      "Epoch 3: Traning Loss: 0.020174183384687232 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:27:12,493] Trial 2 finished with value: 0.013207166455686092 and parameters: {'lr': 0.04085243035135183, 'weight_decay': 5.1814023066376746e-05, 'dropout_p': 0.44841376827852025}. Best is trial 0 with value: 0.005485587287694216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.01131594181060791 WAPE : 43.009027742701036 Validation Loss: 0.01333835069090128\n",
      "Early stopping!\n",
      "Epoch 1: Traning Loss: 0.02021454844366753 NSE : -0.33572936058044434 WAPE : 44.23828475506935 Validation Loss: 0.01761707291007042\n",
      "Epoch 2: Traning Loss: 0.0149258001646552 NSE : 0.17974317073822021 WAPE : 39.98269333548321 Validation Loss: 0.010818450711667538\n",
      "Epoch 3: Traning Loss: 0.012396582317907337 NSE : 0.49580204486846924 WAPE : 25.607819679916165 Validation Loss: 0.006649917922914028\n",
      "Epoch 4: Traning Loss: 0.01194335750285411 NSE : 0.5093318223953247 WAPE : 24.05533910709437 Validation Loss: 0.006471472792327404\n",
      "Epoch 5: Traning Loss: 0.011576157782051788 NSE : 0.5556380748748779 WAPE : 23.356867807913044 Validation Loss: 0.005860734730958939\n",
      "Epoch 6: Traning Loss: 0.011657671941969318 NSE : 0.44455528259277344 WAPE : 29.799771142707872 Validation Loss: 0.0073258159682154655\n",
      "Epoch 7: Traning Loss: 0.011231317609898596 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:27:55,357] Trial 3 finished with value: 0.005860734730958939 and parameters: {'lr': 0.0038696869690797446, 'weight_decay': 0.00012241976861373418, 'dropout_p': 0.21898644223042274}. Best is trial 0 with value: 0.005485587287694216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : 0.48120635747909546 WAPE : 28.915960128778867 Validation Loss: 0.00684242183342576\n",
      "Early stopping!\n",
      "Epoch 1: Traning Loss: 0.016473092762253328 NSE : -0.0011849403381347656 WAPE : 42.03434146690564 Validation Loss: 0.013204730115830898\n",
      "Epoch 2: Traning Loss: 0.017385200417644325 NSE : -4.184246063232422e-05 WAPE : 41.72054089654064 Validation Loss: 0.013189652934670448\n",
      "Epoch 3: Traning Loss: 0.017307955806843637 NSE : -5.8531761169433594e-05 WAPE : 41.57488137941884 Validation Loss: 0.013189874589443207\n",
      "Epoch 4: Traning Loss: 0.017264709648544957 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:28:19,786] Trial 4 finished with value: 0.013189652934670448 and parameters: {'lr': 0.0003027194626768554, 'weight_decay': 0.0008800559582517779, 'dropout_p': 0.28778323694491004}. Best is trial 0 with value: 0.005485587287694216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.00030875205993652344 WAPE : 41.47719865495885 Validation Loss: 0.01319317426532507\n",
      "Early stopping!\n",
      "Number of finished trials: 5\n",
      "Best trial:\n",
      "  Value (Best Validation Loss): 0.005485587287694216\n",
      "  Params:\n",
      "    lr: 0.0004182462427868436\n",
      "    weight_decay: 4.137577281552585e-05\n",
      "    dropout_p: 0.1917303324665699\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Here we define the search space of the hyper-parameters. Optuna uses byaesian optimization to find the optimal values of the hyperparameters.\n",
    "    learning_rate = trial.suggest_loguniform('lr', 1e-4, 1e-1)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n",
    "    dropout_p = trial.suggest_uniform('dropout_p', 0.0, 0.5)\n",
    "    \n",
    "    model = BasicMLPNetwork(seq_len, pred_len, num_features, dropout_p)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    num_epochs = 15\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 2\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = []\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss_val = loss(outputs, labels)\n",
    "    \n",
    "            # calculate gradients for back propagation\n",
    "            loss_val.backward()\n",
    "    \n",
    "            # update the weights based on the gradients\n",
    "            optimizer.step()\n",
    "    \n",
    "            # reset the gradients, avoid gradient accumulation\n",
    "            optimizer.zero_grad()\n",
    "            epoch_loss.append(loss_val.item())\n",
    "    \n",
    "        avg_train_loss = sum(epoch_loss)/len(epoch_loss)\n",
    "        print(f'Epoch {epoch+1}: Traning Loss: {avg_train_loss}', end=' ')\n",
    "        avg_val_loss = evaluate_model(model, val_loader)\n",
    "    \n",
    "        # Check for improvement\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), 'best_model_trial.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print('Early stopping!')\n",
    "                # Load the best model before stopping\n",
    "                model.load_state_dict(torch.load('best_model_trial.pth'))\n",
    "                break\n",
    "\n",
    "        # Report intermediate objective value\n",
    "        trial.report(best_val_loss, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "# normally you run 100s of trials.\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "print('  Value (Best Validation Loss):', trial.value)\n",
    "print('  Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7216e93-a35d-4aa5-83db-2e43ea380f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to install plotly and nbformat to plot in jupyter notebook\n",
    "# I'm not going to the details. But you can visualize many many things.\n",
    "# source: https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/005_visualization.html\n",
    "\n",
    "import optuna.visualization as vis\n",
    "\n",
    "# Optimization history\n",
    "fig1 = vis.plot_optimization_history(study)\n",
    "fig1.write_html(\"optimization_history_mlp.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3adcb080-98f7-4dfa-bc75-298d94becc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FrozenTrial(number=0, state=TrialState.COMPLETE, values=[0.005485587287694216], datetime_start=datetime.datetime(2024, 11, 19, 16, 24, 36, 665900), datetime_complete=datetime.datetime(2024, 11, 19, 16, 26, 11, 321492), params={'lr': 0.0004182462427868436, 'weight_decay': 4.137577281552585e-05, 'dropout_p': 0.1917303324665699}, user_attrs={}, system_attrs={}, intermediate_values={0: 0.013396549969911575, 1: 0.013396549969911575, 2: 0.012537333182990551, 3: 0.0070018162950873375, 4: 0.006255871616303921, 5: 0.0060210018418729305, 6: 0.00592461321502924, 7: 0.005825395230203867, 8: 0.005693289451301098, 9: 0.005661779083311558, 10: 0.005582711659371853, 11: 0.005526938010007143, 12: 0.005526938010007143, 13: 0.005485587287694216, 14: 0.005485587287694216}, distributions={'lr': FloatDistribution(high=0.1, log=True, low=0.0001, step=None), 'weight_decay': FloatDistribution(high=0.01, log=True, low=1e-05, step=None), 'dropout_p': FloatDistribution(high=0.5, log=False, low=0.0, step=None)}, trial_id=0, value=None),\n",
       " FrozenTrial(number=1, state=TrialState.COMPLETE, values=[0.013192687183618546], datetime_start=datetime.datetime(2024, 11, 19, 16, 26, 11, 322144), datetime_complete=datetime.datetime(2024, 11, 19, 16, 26, 54, 188905), params={'lr': 0.07743412458199854, 'weight_decay': 1.4788727760527778e-05, 'dropout_p': 0.13864611869658144}, user_attrs={}, system_attrs={}, intermediate_values={0: 0.027376465499401093, 1: 0.017123490571975708, 2: 0.013616050593554974, 3: 0.013345684856176376, 4: 0.013192687183618546, 5: 0.013192687183618546}, distributions={'lr': FloatDistribution(high=0.1, log=True, low=0.0001, step=None), 'weight_decay': FloatDistribution(high=0.01, log=True, low=1e-05, step=None), 'dropout_p': FloatDistribution(high=0.5, log=False, low=0.0, step=None)}, trial_id=1, value=None),\n",
       " FrozenTrial(number=2, state=TrialState.COMPLETE, values=[0.013207166455686092], datetime_start=datetime.datetime(2024, 11, 19, 16, 26, 54, 189497), datetime_complete=datetime.datetime(2024, 11, 19, 16, 27, 12, 493547), params={'lr': 0.04085243035135183, 'weight_decay': 5.1814023066376746e-05, 'dropout_p': 0.44841376827852025}, user_attrs={}, system_attrs={}, intermediate_values={0: 0.013207166455686092, 1: 0.013207166455686092}, distributions={'lr': FloatDistribution(high=0.1, log=True, low=0.0001, step=None), 'weight_decay': FloatDistribution(high=0.01, log=True, low=1e-05, step=None), 'dropout_p': FloatDistribution(high=0.5, log=False, low=0.0, step=None)}, trial_id=2, value=None),\n",
       " FrozenTrial(number=3, state=TrialState.COMPLETE, values=[0.005860734730958939], datetime_start=datetime.datetime(2024, 11, 19, 16, 27, 12, 494121), datetime_complete=datetime.datetime(2024, 11, 19, 16, 27, 55, 357205), params={'lr': 0.0038696869690797446, 'weight_decay': 0.00012241976861373418, 'dropout_p': 0.21898644223042274}, user_attrs={}, system_attrs={}, intermediate_values={0: 0.01761707291007042, 1: 0.010818450711667538, 2: 0.006649917922914028, 3: 0.006471472792327404, 4: 0.005860734730958939, 5: 0.005860734730958939}, distributions={'lr': FloatDistribution(high=0.1, log=True, low=0.0001, step=None), 'weight_decay': FloatDistribution(high=0.01, log=True, low=1e-05, step=None), 'dropout_p': FloatDistribution(high=0.5, log=False, low=0.0, step=None)}, trial_id=3, value=None),\n",
       " FrozenTrial(number=4, state=TrialState.COMPLETE, values=[0.013189652934670448], datetime_start=datetime.datetime(2024, 11, 19, 16, 27, 55, 357812), datetime_complete=datetime.datetime(2024, 11, 19, 16, 28, 19, 785935), params={'lr': 0.0003027194626768554, 'weight_decay': 0.0008800559582517779, 'dropout_p': 0.28778323694491004}, user_attrs={}, system_attrs={}, intermediate_values={0: 0.013204730115830898, 1: 0.013189652934670448, 2: 0.013189652934670448}, distributions={'lr': FloatDistribution(high=0.1, log=True, low=0.0001, step=None), 'weight_decay': FloatDistribution(high=0.01, log=True, low=1e-05, step=None), 'dropout_p': FloatDistribution(high=0.5, log=False, low=0.0, step=None)}, trial_id=4, value=None)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.trials"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
