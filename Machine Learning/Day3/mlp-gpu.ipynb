{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ccae883-1904-48a6-81b3-d0228caeab1f",
   "metadata": {},
   "source": [
    "### Multivariate time series prediction using MLP\n",
    "\n",
    "\n",
    "Here is a pytorch implementation of a fully connected neural network which has in total 4 fully connected layers. This is a vanilla\n",
    "neural network model to be used for multi variate time series prediction. The data we have is 2D, the model learns first\n",
    "along the time axis and secondly it learns along the feature axis. It has two fully connected layers with size\n",
    "**seq_len x 100** and **100 x pred_len** with **ReLU** loss function along time axis and two fully connected layers with sizes **num_features x 20** and **20 x 1** with same **ReLU** as loss function along the features axis. The neural network uses Mean Square Error Loss (MSELoss) to calculate the model loss. NSE and WAPE are used for model evaluation. The data is normalized using StandardScaler from scikit learn. Data is used is the 3 hr river gauge height data. It is divided in the ratio of 7:1:2 training, validation and testing respectively.\n",
    "\n",
    "This notebooks implements MLP with **regularization and early stopping** with model implemented to fully use **GPU**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4117e9d0-7925-4788-bbe1-28c60c80bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8af4f85b-3822-4288-9566-0f0ac0cfa053",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../dataset/final_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc9eb646-4308-449c-84c0-00cd1de182c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Implement determinism. Set a fixed value for random seed so that when the parameters are initialized, they are initialized same across all experiments.\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# If you are using CUDA, also set the seed for it\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Set the seed for NumPy\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0600e784-ec1c-4ab8-ab6b-9b765b4dee83",
   "metadata": {},
   "source": [
    "Here we define **RiverData** a custom Dataset class to load the dataset we have. It extends the pytorch Dataset class.  \n",
    "- We need to define \\_\\_init__() function which can be used for loading data from file and optionally for data preprocessing.\n",
    "- Thereafter we define \\_\\_len__() function which gives the length of dataset.\n",
    "- Then we define \\_\\_getitem__() function which returns an instance of (feature, label) tuple which can be used for model training.\n",
    "  For our time series data, feature means the past values to be used for training and label means the future values to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "742b694d-9966-4fd3-bca7-51f398a8775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiverData(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, df, target, datecol, seq_len, pred_len):\n",
    "        self.df = df\n",
    "        self.datecol = datecol\n",
    "        self.target = target\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.setIndex()\n",
    "        \n",
    "\n",
    "    def setIndex(self):\n",
    "        self.df.set_index(self.datecol, inplace=True)\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) - self.seq_len - self.pred_len\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if len(self.df) <= (idx + self.seq_len+self.pred_len):\n",
    "            raise IndexError(f\"Index {idx} is out of bounds for dataset of size {len(self.df)}\")\n",
    "        df_piece = self.df[idx:idx+self.seq_len].values\n",
    "        feature = torch.tensor(df_piece, dtype=torch.float32)\n",
    "        label_piece = self.df[self.target][idx + self.seq_len:  idx+self.seq_len+self.pred_len].values\n",
    "        label = torch.tensor(label_piece, dtype=torch.float32)\n",
    "        return (feature.T, label) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c24959-e72f-435d-8a14-69a648fa5ab1",
   "metadata": {},
   "source": [
    "### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d897d9-946e-4bbe-90a6-cc8d8a223c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)\n",
    "df = df[df['DATE'] > '2012']\n",
    "\n",
    "raw_df = df.drop('DATE', axis=1, inplace=False)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply the transformations\n",
    "df_scaled = scaler.fit_transform(raw_df)\n",
    "\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=raw_df.columns)\n",
    "df_scaled['DATE'] = df['DATE']\n",
    "df = df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4781c6-88ed-4b28-aa0e-f669bf7c75f5",
   "metadata": {},
   "source": [
    "Some advanced python syntax have been used here. \\\n",
    "*common_args : it's used to pass arguments to a function, where common_args represents a python list \\\n",
    "**common_args: it's used to pass arguments to a function, where common_args represents a python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e63a535-a817-4429-be17-f3e3a02e4f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_size = int(0.7 * len(df))\n",
    "test_size = int(0.2 * len(df))\n",
    "val_size = len(df) - train_size - test_size\n",
    "\n",
    "seq_len = 13\n",
    "pred_len = 1\n",
    "num_features = 7\n",
    "\n",
    "common_args = ['gauge_height', 'DATE', seq_len, pred_len]\n",
    "train_dataset = RiverData(df[:train_size], *common_args)\n",
    "val_dataset = RiverData(df[train_size: train_size+val_size], *common_args)\n",
    "test_dataset = RiverData(df[train_size+val_size : len(df)], *common_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "501879b1-dd1c-4ff0-afb3-ff25a345df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important parameters\n",
    "\n",
    "BATCH_SIZE = 512 # keep as big as can be handled by GPU and memory\n",
    "SHUFFLE = False # we don't shuffle the time series data\n",
    "DATA_LOAD_WORKERS = 1 # it depends on amount of data you need to load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae359aec-9435-4229-8587-5f120b0370b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "common_args = {'batch_size': BATCH_SIZE, 'shuffle': SHUFFLE}\n",
    "train_loader = DataLoader(train_dataset, **common_args)\n",
    "val_loader = DataLoader(val_dataset, **common_args)\n",
    "test_loader = DataLoader(test_dataset, **common_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ac35c-8ecc-4ab7-a97c-0cb4d59bc624",
   "metadata": {},
   "source": [
    "### Here we define our pytorch model.\n",
    "\n",
    "BasicMLPNetwork is the model class, it extends the Module class provided by pytorch. \\\n",
    "- We define \\_\\_init__() function. It sets up layers and defines the model parameters.\n",
    "- Also, we define forward() function which defines how the forwared pass computation occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9b72d79-65b6-4fd0-9d0b-025c2a0fc293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are adding dropout layers.\n",
    "\n",
    "class BasicMLPNetwork(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len, pred_len, num_features, dropout):\n",
    "        # call the constructor of the base class\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.num_features = num_features\n",
    "        hidden_size_time = 512\n",
    "        hidden_size_feat = 128\n",
    "        # define layers for combining across time series\n",
    "        self.fc1 = torch.nn.Linear(self.seq_len, hidden_size_time)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout1 = torch.nn.Dropout(p=dropout)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size_time, self.pred_len)\n",
    "        self.dropout2 = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        # define layers for combining across the features\n",
    "        self.fc3 = torch.nn.Linear(self.num_features, hidden_size_feat)\n",
    "        self.fc4 = torch.nn.Linear(hidden_size_feat, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # computation over time\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out) # has dimension 512 x 7 x 12\n",
    "        out = self.dropout2(out)\n",
    "        # computation over features\n",
    "        out = out.transpose(1,2) # dimension 512 x 12 x 7\n",
    "        out = self.fc3(out) # dimension 512 x 12 x 20\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out) # dimension 512 x 12 x 1\n",
    "\n",
    "        out = out.squeeze(-1) # dimension 512 x 12\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Note that the gradients are stored insize the FC layer objects\n",
    "# For each training example we need to get rid of these gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5277794b-fe37-4595-8554-d26db5710e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_p = 0.189369\n",
    "learning_rate = 0.000294\n",
    "weight_decay = 0.00056  # Regularization strength\n",
    "\n",
    "model = BasicMLPNetwork(seq_len, pred_len, num_features, dropout_p)\n",
    "model = model.to(device)\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1afde643-9953-4129-8415-7b4836c31300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 13])\n",
      "torch.Size([512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1])\n",
      "torch.Size([128, 7])\n",
      "torch.Size([128])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for gen in model.parameters():\n",
    "    print(gen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b3cc7a8-9844-4eae-a601-4787513eb1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape:  torch.Size([512, 7, 13])\n",
      "labels shape:  torch.Size([512, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, (f,l) in enumerate(train_loader):\n",
    "    print('features shape: ', f.shape)\n",
    "    print('labels shape: ', l.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22683e91-642a-494b-abc5-069dc6fa9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics\n",
    "import numpy as np\n",
    "epsilon = np.finfo(float).eps\n",
    "\n",
    "def Wape(y, y_pred):\n",
    "    \"\"\"Weighted Average Percentage Error metric in the interval [0; 100]\"\"\"\n",
    "    y = np.array(y)\n",
    "    y_pred = np.array(y_pred)\n",
    "    nominator = np.sum(np.abs(np.subtract(y, y_pred)))\n",
    "    denominator = np.add(np.sum(np.abs(y)), epsilon)\n",
    "    wape = np.divide(nominator, denominator) * 100.0\n",
    "    return wape\n",
    "\n",
    "def nse(y, y_pred):\n",
    "    y = np.array(y)\n",
    "    y_pred = np.array(y_pred)\n",
    "    return (1-(np.sum((y_pred-y)**2)/np.sum((y-np.mean(y))**2)))\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    # following line prepares the model for evaulation mode. It disables dropout and batch normalization if they have \n",
    "    # are part of the model. For our simple model it's not necessary. Still I'm going to use it.\n",
    "\n",
    "    model.eval()\n",
    "    all_inputs = torch.empty((0, num_features, seq_len))\n",
    "    all_labels = torch.empty(0, pred_len)\n",
    "    for inputs, labels in data_loader:\n",
    "        all_inputs = torch.vstack((all_inputs, inputs))\n",
    "        all_labels = torch.vstack((all_labels, labels))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        all_inputs = all_inputs.to(device)\n",
    "        outputs = model(all_inputs).detach().cpu()\n",
    "        avg_val_loss = loss(outputs, all_labels)\n",
    "        nsee = nse(all_labels.numpy(), outputs.numpy())\n",
    "        wapee = Wape(all_labels.numpy(), outputs.numpy())\n",
    "        \n",
    "    print(f'NSE : {nsee}', end=' ')\n",
    "    print(f'WAPE : {wapee}', end=' ')\n",
    "    print(f'Validation Loss: {avg_val_loss}')\n",
    "    model.train()\n",
    "    return avg_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9beaf3a1-2c40-4fc6-b0f7-c352b5763231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Traning Loss: 0.01744051495857612 NSE : -0.0004334449768066406 WAPE : 41.44640688705754 Validation Loss: 0.013194818049669266\n",
      "Epoch 2: Traning Loss: 0.01723201509921393 NSE : -0.0007778406143188477 WAPE : 41.38129416191354 Validation Loss: 0.01319936104118824\n",
      "Epoch 3: Traning Loss: 0.017206342720532876 NSE : -0.001185297966003418 WAPE : 41.32305981043181 Validation Loss: 0.013204734772443771\n",
      "Epoch 4: Traning Loss: 0.017190685550425976 NSE : -0.001508951187133789 WAPE : 41.284773288040434 Validation Loss: 0.013209004886448383\n",
      "Epoch 5: Traning Loss: 0.01717302050510259 NSE : -0.0019409656524658203 WAPE : 41.24069933150499 Validation Loss: 0.013214699923992157\n",
      "Epoch 6: Traning Loss: 0.01714886643536289 NSE : -0.0027016401290893555 WAPE : 41.17602570951706 Validation Loss: 0.013224733993411064\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = []\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss_val = loss(outputs, labels)\n",
    "\n",
    "        # calculate gradients for back propagation\n",
    "        loss_val.backward()\n",
    "\n",
    "        # update the weights based on the gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # reset the gradients, avoid gradient accumulation\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss.append(loss_val.item())\n",
    "\n",
    "    avg_train_loss = sum(epoch_loss)/len(epoch_loss)\n",
    "    print(f'Epoch {epoch+1}: Traning Loss: {avg_train_loss}', end=' ')\n",
    "    avg_val_loss = evaluate_model(model, val_loader)\n",
    "\n",
    "    # Check for improvement\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == patience:\n",
    "            print('Early stopping!')\n",
    "            # Load the best model before stopping\n",
    "            model.load_state_dict(torch.load('best_model.pth'))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7216e93-a35d-4aa5-83db-2e43ea380f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.1523803472518921 WAPE : 58.526007822577576 Validation Loss: 0.01285931933671236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0129)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adcb080-98f7-4dfa-bc75-298d94becc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa21e49-a774-4a21-ba77-aad2d3617f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6736f873-ed65-4d8c-942e-fc227c1cae0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
