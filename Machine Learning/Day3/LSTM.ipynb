{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ccae883-1904-48a6-81b3-d0228caeab1f",
   "metadata": {},
   "source": [
    "## Multivariate time series prediction using Long Short Term Memory (LSTM)\n",
    "\n",
    "### Theory of Recurrent Neural Network (RNN)\n",
    "\n",
    "The most important difference between RNN and Fully Connected Network(FCN) is that RNN does sequential processing of input data and maintains a hidden state.\n",
    "The hidden state is updated for each sequence input. When we train the model, it learns how to update the hidden state in the best possible way such that it can be used to predict the next value in the sequence. Basic example:\n",
    "\n",
    "### Let's consider a network that learns to count the number of 1's in a binary sequence.\n",
    "\n",
    "So, in an FCN, we feed the entire binary sequence at once, e.g. 1, 0, 1, 1 is fed at once, and it multiplies with a weight matrix of a 4x4 matrix and the computes flows forward.\n",
    "\n",
    "But, in a LSTM, \n",
    "- Model first takes 1 and an initial hidden state h which is normally assigned to be 0, updates the hidden state h,\n",
    "- Then takes 0 and h and updates hidden state h,\n",
    "- Then takes 1 and h updates hidden state h,\n",
    "- Also finally takes 1 and h and updates the hidden state h.\n",
    "\n",
    "  This final h value is used to predict the number of 1's in the binary sequence. Normally this h is passed through a fully connected layer to get the required final output.\n",
    "\n",
    "Mathematically,\n",
    "\n",
    "$$ h_t = W_{xh}x_t + b_t + W_{hh} h_{t-1} + b_h $$\n",
    "\n",
    "Final output is calculated as\n",
    "$$ y_t = h_t W_{hy}^T + b_y $$\n",
    "\n",
    "Here is a PyTorch implementation of an LSTM neural network which has 1 LSTM layer and 1 fully connected layer.\n",
    "\n",
    "\n",
    "### Long Short Term Memory (LSTM)\n",
    "\n",
    "LSTM is basically an RNN where the way we calculate the hidden state is more fancy and involves multiple equations. We use LSTM because vanilla RNN has a tendency to suffer from vanishing gradient problem. In practice LSTM have been observed to work much better. Pytorch implementation of LSTM uses following equations in each layer.\n",
    "\n",
    "We initialize the $h_{t-1}$ and $c_{t-1}$ to zeros in every forward pass\n",
    "$$\\begin{array}{ll} \\\\\n",
    "         Input Gate:   i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n",
    "         Forget Gate:   f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
    "         Candidate Cell State:   g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n",
    "         Output Gate:   o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n",
    "         Cell State Update:   c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "         Hidden State:   h_t = o_t \\odot \\tanh(c_t) \\\\\n",
    "        \\end{array}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4117e9d0-7925-4788-bbe1-28c60c80bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8af4f85b-3822-4288-9566-0f0ac0cfa053",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../dataset/chattahoochee_3hr_02336490.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc9eb646-4308-449c-84c0-00cd1de182c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0600e784-ec1c-4ab8-ab6b-9b765b4dee83",
   "metadata": {},
   "source": [
    "Here we define **RiverData** a custom Dataset class to load the dataset we have. It extends the pytorch Dataset class.  \n",
    "- We need to define \\_\\_init__() function which can be used for loading data from file and optionally for data preprocessing.\n",
    "- Thereafter we define \\_\\_len__() function which gives the length of dataset.\n",
    "- Then we define \\_\\_getitem__() function which returns an instance of (feature, label) tuple which can be used for model training.\n",
    "  For our time series data, feature means the past values to be used for training and label means the future values to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "742b694d-9966-4fd3-bca7-51f398a8775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiverData(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, df, target, datecol, seq_len, pred_len):\n",
    "        self.df = df\n",
    "        self.datecol = datecol\n",
    "        self.target = target\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.setIndex()\n",
    "        \n",
    "\n",
    "    def setIndex(self):\n",
    "        self.df.set_index(self.datecol, inplace=True)\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) - self.seq_len - self.pred_len\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if len(self.df) <= (idx + self.seq_len+self.pred_len):\n",
    "            raise IndexError(f\"Index {idx} is out of bounds for dataset of size {len(self.df)}\")\n",
    "        df_piece = self.df[idx:idx+self.seq_len].values\n",
    "        feature = torch.tensor(df_piece, dtype=torch.float32)\n",
    "        label_piece = self.df[self.target][idx + self.seq_len:  idx+self.seq_len+self.pred_len].values\n",
    "        label = torch.tensor(label_piece, dtype=torch.float32)\n",
    "        return (feature, label) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c24959-e72f-435d-8a14-69a648fa5ab1",
   "metadata": {},
   "source": [
    "### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d897d9-946e-4bbe-90a6-cc8d8a223c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)\n",
    "raw_df = df.drop('DATE', axis=1, inplace=False)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply the transformations\n",
    "df_scaled = scaler.fit_transform(raw_df)\n",
    "\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=raw_df.columns)\n",
    "df_scaled['DATE'] = df['DATE']\n",
    "df = df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4781c6-88ed-4b28-aa0e-f669bf7c75f5",
   "metadata": {},
   "source": [
    "Some advanced python syntax have been used here. \\\n",
    "*common_args : it's used to pass arguments to a function, where common_args represents a python list \\\n",
    "**common_args: it's used to pass arguments to a function, where common_args represents a python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e63a535-a817-4429-be17-f3e3a02e4f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_size = int(0.7 * len(df))\n",
    "test_size = int(0.2 * len(df))\n",
    "val_size = len(df) - train_size - test_size\n",
    "\n",
    "seq_len = 8\n",
    "pred_len = 1\n",
    "num_features = 7\n",
    "\n",
    "common_args = ['gaze_height', 'DATE', seq_len, pred_len]\n",
    "train_dataset = RiverData(df[:train_size], *common_args)\n",
    "val_dataset = RiverData(df[train_size: train_size+val_size], *common_args)\n",
    "test_dataset = RiverData(df[train_size+val_size : len(df)], *common_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "501879b1-dd1c-4ff0-afb3-ff25a345df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important parameters\n",
    "\n",
    "BATCH_SIZE = 512 # keep as big as can be handled by GPU and memory\n",
    "SHUFFLE = False # we don't shuffle the time series data\n",
    "DATA_LOAD_WORKERS = 1 # it depends on amount of data you need to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae359aec-9435-4229-8587-5f120b0370b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "common_args = {'batch_size': BATCH_SIZE, 'shuffle': SHUFFLE}\n",
    "train_loader = DataLoader(train_dataset, **common_args)\n",
    "val_loader = DataLoader(val_dataset, **common_args)\n",
    "test_loader = DataLoader(test_dataset, **common_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ac35c-8ecc-4ab7-a97c-0cb4d59bc624",
   "metadata": {},
   "source": [
    "### Here we define our pytorch model.\n",
    "\n",
    "BasicLSTMNetwork is the model class, it extends the Module class provided by pytorch. \\\n",
    "- We define \\_\\_init__() function. It sets up layers and defines the model parameters.\n",
    "- Also, we define forward() function which defines how the forwared pass computation occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01cd8d0d-2058-4dfe-9046-78cde5fd2f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLSTMNetwork(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len, pred_len):\n",
    "        # call the constructor of the base class\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.num_features = num_features\n",
    "        self.n_layers = 1\n",
    "        \n",
    "        self.n_hidden = 128\n",
    "        \n",
    "        # define layers for combining across time series\n",
    "        self.lstm1 = torch.nn.LSTM(input_size = self.num_features, hidden_size = self.n_hidden, num_layers=self.n_layers, batch_first = True)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc1 = torch.nn.Linear(self.n_hidden * self.seq_len, self.pred_len)\n",
    "\n",
    "\n",
    "    def init_hidden(self, batchsize):\n",
    "        device = next(self.parameters()).device\n",
    "        hidden_state = torch.zeros(self.n_layers, batchsize, self.n_hidden, device=device)\n",
    "        cell_state = torch.zeros(self.n_layers, batchsize, self.n_hidden, device=device)\n",
    "        return hidden_state, cell_state\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batchsize, seqlen, featlen = x.size()\n",
    "        self.hidden_states = self.init_hidden(batchsize)\n",
    "        lstm_out, self.hidden_states = self.lstm1(x, self.hidden_states)\n",
    "        lstm_out = lstm_out.contiguous().view(batchsize, -1)\n",
    "        lstm_out = self.relu(lstm_out)\n",
    "        lstm_out = self.fc1(lstm_out)\n",
    "        return lstm_out\n",
    "# Note that the gradients are stored insize the FC layer objects\n",
    "# For each training example we need to get rid of these gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f579dc18-584e-414b-ae8e-37f62e4b42f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b600c877-3409-48f9-80e5-3fdde7731817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5277794b-fe37-4595-8554-d26db5710e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicLSTMNetwork(seq_len, pred_len)\n",
    "loss = torch.nn.MSELoss()\n",
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1afde643-9953-4129-8415-7b4836c31300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 7])\n",
      "torch.Size([512, 128])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for gen in model.parameters():\n",
    "    print(gen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b3cc7a8-9844-4eae-a601-4787513eb1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape:  torch.Size([512, 8, 7])\n",
      "labels shape:  torch.Size([512, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, (f,l) in enumerate(train_loader):\n",
    "    print('features shape: ', f.shape)\n",
    "    print('labels shape: ', l.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22683e91-642a-494b-abc5-069dc6fa9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics\n",
    "import numpy as np\n",
    "epsilon = np.finfo(float).eps\n",
    "\n",
    "def Wape(y, y_pred):\n",
    "    \"\"\"Weighted Average Percentage Error metric in the interval [0; 100]\"\"\"\n",
    "    y = np.array(y)\n",
    "    y_pred = np.array(y_pred)\n",
    "    nominator = np.sum(np.abs(np.subtract(y, y_pred)))\n",
    "    denominator = np.add(np.sum(np.abs(y)), epsilon)\n",
    "    wape = np.divide(nominator, denominator) * 100.0\n",
    "    return wape\n",
    "\n",
    "def nse(y, y_pred):\n",
    "    y = np.array(y)\n",
    "    y_pred = np.array(y_pred)\n",
    "    return (1-(np.sum((y_pred-y)**2)/np.sum((y-np.mean(y))**2)))\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    # following line prepares the model for evaulation mode. It disables dropout and batch normalization if they have \n",
    "    # are part of the model. For our simple model it's not necessary. Still I'm going to use it.\n",
    "\n",
    "    model.eval()\n",
    "    all_inputs = torch.empty((0, seq_len, num_features))\n",
    "    all_labels = torch.empty(0, pred_len)\n",
    "    for inputs, labels in data_loader:\n",
    "        all_inputs = torch.vstack((all_inputs, inputs))\n",
    "        all_labels = torch.vstack((all_labels, labels))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(all_inputs)\n",
    "        nsee = nse(all_labels.numpy(), outputs.numpy())\n",
    "        wapee = Wape(all_labels.numpy(), outputs.numpy())\n",
    "        \n",
    "    print(f'NSE : {nsee}', end=' ')\n",
    "    print(f'WAPE : {wapee}')\n",
    "    \n",
    "    model.train()\n",
    "    return nsee, wapee\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9beaf3a1-2c40-4fc6-b0f7-c352b5763231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 0.4418947807802209 NSE : 0.720867931842804 WAPE : 47.475956760293606\n",
      "Epoch 2: 0.1927654768243946 NSE : 0.7731529474258423 WAPE : 43.65733243049667\n",
      "Epoch 3: 0.15903812772112674 NSE : 0.799297034740448 WAPE : 41.308563544733104\n",
      "Epoch 4: 0.1392704789880021 NSE : 0.8164103031158447 WAPE : 39.51131467659179\n",
      "Epoch 5: 0.12540071157353191 NSE : 0.8299570679664612 WAPE : 37.95272229365697\n",
      "Epoch 6: 0.11464775135291033 NSE : 0.8416422009468079 WAPE : 36.524090814592846\n",
      "Epoch 7: 0.10591341429871731 NSE : 0.8520503044128418 WAPE : 35.21430205360582\n",
      "Epoch 8: 0.09865662213093762 NSE : 0.8613989353179932 WAPE : 34.01451555405517\n",
      "Epoch 9: 0.09252688246939716 NSE : 0.8697718977928162 WAPE : 32.90339104666705\n",
      "Epoch 10: 0.08730316894321606 NSE : 0.8772249817848206 WAPE : 31.89096487735129\n",
      "Epoch 11: 0.08283493672658143 NSE : 0.8837827444076538 WAPE : 30.9748094234771\n",
      "Epoch 12: 0.07901208109928873 NSE : 0.8895360827445984 WAPE : 30.155879441327453\n",
      "Epoch 13: 0.07571976317157006 NSE : 0.8945804834365845 WAPE : 29.425608192786473\n",
      "Epoch 14: 0.07286698950839968 NSE : 0.8990076184272766 WAPE : 28.774294962992602\n",
      "Epoch 15: 0.07037428600296121 NSE : 0.9029008746147156 WAPE : 28.1895279202671\n",
      "Epoch 16: 0.06816722069674268 NSE : 0.9063352346420288 WAPE : 27.64895664363738\n",
      "Epoch 17: 0.06620540363906786 NSE : 0.9094029664993286 WAPE : 27.157682399181972\n",
      "Epoch 18: 0.06443053464694269 NSE : 0.9121484756469727 WAPE : 26.713360657393693\n",
      "Epoch 19: 0.06281806675492432 NSE : 0.9146168231964111 WAPE : 26.307618564111724\n",
      "Epoch 20: 0.061338043619139956 NSE : 0.9168530106544495 WAPE : 25.932666546317414\n",
      "Epoch 21: 0.05997413970080429 NSE : 0.9188900589942932 WAPE : 25.583509240932912\n",
      "Epoch 22: 0.058708730659543955 NSE : 0.9207630157470703 WAPE : 25.256661624597648\n",
      "Epoch 23: 0.057531295320147585 NSE : 0.9224845767021179 WAPE : 24.954798970036293\n",
      "Epoch 24: 0.05643182718355594 NSE : 0.9240725040435791 WAPE : 24.67309535897207\n",
      "Epoch 25: 0.0554035688140269 NSE : 0.9255407452583313 WAPE : 24.40887877723418\n",
      "Epoch 26: 0.05444262660076392 NSE : 0.926904022693634 WAPE : 24.16013543853292\n",
      "Epoch 27: 0.053540773304372 NSE : 0.9281806945800781 WAPE : 23.924356256390837\n",
      "Epoch 28: 0.052694563994762196 NSE : 0.9293723106384277 WAPE : 23.6989164656682\n",
      "Epoch 29: 0.05189386930252458 NSE : 0.9304812550544739 WAPE : 23.48448406990771\n",
      "Epoch 30: 0.051148133047310446 NSE : 0.9315215349197388 WAPE : 23.280158078965187\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = []\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss_val = loss(outputs, labels)\n",
    "\n",
    "        # calculate gradients for back propagation\n",
    "        loss_val.backward()\n",
    "\n",
    "        # update the weights based on the gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # reset the gradients, avoid gradient accumulation\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss.append(loss_val.item())\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: {sum(epoch_loss)/len(epoch_loss)}', end=' ')\n",
    "    nsee, wapee = evaluate_model(model, val_loader)\n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7216e93-a35d-4aa5-83db-2e43ea380f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : 0.9128164649009705 WAPE : 25.942366506818125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float32(0.91281646), np.float64(25.942366506818125))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722fee71-de14-463f-abf6-e35f9f002ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2f592f-d2be-4ea2-9e9e-d034d45f551d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
