{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ccae883-1904-48a6-81b3-d0228caeab1f",
   "metadata": {},
   "source": [
    "### Multivariate time series prediction using MLP\n",
    "\n",
    "\n",
    "Here is a PyTorch implementation of a fully connected neural network which has in total 4 fully connected layers. This is a vanilla\n",
    "neural network model to be used for multivariate time series prediction. The model has two fully connected layers with size\n",
    "**seq_len x 100** and **100 x pred_len** with **ReLU** loss function along a time axis and two fully connected layers with sizes **num_features x 20** and **20 x 1** with same **ReLU** as loss function along the features axis. The neural network uses Mean Square Error Loss (MSELoss) to calculate the model loss. NSE and WAPE are used for model evaluation. The data is normalized using MinMaxScaler from scikit learn. Data is used is the 3 hr river gauge height data. It is divided in the ratio of 7:1:2 training, validation and testing respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4117e9d0-7925-4788-bbe1-28c60c80bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write the above description in more organized way, like\n",
    "# No. of layers =  4\n",
    "# Sequence length = 24, etc.\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8af4f85b-3822-4288-9566-0f0ac0cfa053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use hourly data, use a subset to reduce the datasize\n",
    "\n",
    "path = '../dataset/final_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc9eb646-4308-449c-84c0-00cd1de182c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x112e14990>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement determinism. Set a fixed value for random seed so that when the parameters are initialized, they are initialized same across all experiments.\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0600e784-ec1c-4ab8-ab6b-9b765b4dee83",
   "metadata": {},
   "source": [
    "Here we define **RiverData** a custom Dataset class to load the dataset we have. It extends the pytorch's **Dataset** class.  \n",
    "- We need to define \\_\\_init__() function which can be used for loading data from file and optionally for data preprocessing.\n",
    "- Thereafter we define \\_\\_len__() function which gives the length of dataset.\n",
    "- Then we define \\_\\_getitem__() function which returns an instance of (feature, label) tuple which can be used for model training.\n",
    "  For our time series data, feature means the past values to be used for training and label means the future values to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "742b694d-9966-4fd3-bca7-51f398a8775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to put in pre-requisite the knowledge for object oriented programming\n",
    "# send emails to partcipants to go through it.\n",
    "\n",
    "class RiverData(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, df, target, datecol, seq_len, pred_len):\n",
    "        self.df = df\n",
    "        self.datecol = datecol\n",
    "        self.target = target\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.setIndex()\n",
    "        \n",
    "\n",
    "    def setIndex(self):\n",
    "        self.df.set_index(self.datecol, inplace=True)\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) - self.seq_len - self.pred_len\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if len(self.df) <= (idx + self.seq_len+self.pred_len):\n",
    "            raise IndexError(f\"Index {idx} is out of bounds for dataset of size {len(self.df)}\")\n",
    "        df_piece = self.df[idx:idx+self.seq_len].values\n",
    "        feature = torch.tensor(df_piece, dtype=torch.float32)\n",
    "        label_piece = self.df[self.target][idx + self.seq_len:  idx+self.seq_len+self.pred_len].values\n",
    "        label = torch.tensor(label_piece, dtype=torch.float32)\n",
    "        return (feature.T, label) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c24959-e72f-435d-8a14-69a648fa5ab1",
   "metadata": {},
   "source": [
    "### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d897d9-946e-4bbe-90a6-cc8d8a223c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)\n",
    "df.reset_index(inplace=True)\n",
    "df = df[df['DATE'] > '2012']\n",
    "\n",
    "df.drop('index', axis=1, inplace=True)\n",
    "raw_df = df.drop('DATE', axis=1, inplace=False)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply the transformations\n",
    "df_scaled = scaler.fit_transform(raw_df)\n",
    "\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=raw_df.columns)\n",
    "df_scaled['DATE'] = df['DATE']\n",
    "df = df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d68a3fc-bfd6-44dd-adfc-211550c864ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a4781c6-88ed-4b28-aa0e-f669bf7c75f5",
   "metadata": {},
   "source": [
    "Some advanced python syntax have been used here. \\\n",
    "*common_args : it's used to pass arguments to a function, where common_args represents a python list \\\n",
    "**common_args: it's used to pass arguments to a function, where common_args represents a python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e63a535-a817-4429-be17-f3e3a02e4f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio for train:test:validation is 7:2:1\n",
    "# we can vary it depending on the dataset\n",
    "\n",
    "train_size = int(0.7 * len(df))\n",
    "test_size = int(0.2 * len(df))\n",
    "val_size = len(df) - train_size - test_size\n",
    "\n",
    "seq_len = 13\n",
    "pred_len = 1\n",
    "num_features = 7\n",
    "\n",
    "common_args = ['gauge_height', 'DATE', seq_len, pred_len]\n",
    "train_dataset = RiverData(df[:train_size], *common_args)\n",
    "val_dataset = RiverData(df[train_size: train_size+val_size], *common_args)\n",
    "test_dataset = RiverData(df[train_size+val_size : len(df)], *common_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "501879b1-dd1c-4ff0-afb3-ff25a345df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important hyperparameters\n",
    "\n",
    "BATCH_SIZE = 512 # keep as big as can be handled by GPU and memory\n",
    "SHUFFLE = False # we don't shuffle the time series data\n",
    "DATA_LOAD_WORKERS = 1 # it depends on amount of data you need to load\n",
    "learning_rate = 1e-3 # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae359aec-9435-4229-8587-5f120b0370b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "common_args = {'batch_size': BATCH_SIZE, 'shuffle': SHUFFLE}\n",
    "train_loader = DataLoader(train_dataset, **common_args)\n",
    "val_loader = DataLoader(val_dataset, **common_args)\n",
    "test_loader = DataLoader(test_dataset, **common_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ac35c-8ecc-4ab7-a97c-0cb4d59bc624",
   "metadata": {},
   "source": [
    "### Here we define our pytorch model.\n",
    "\n",
    "BasicMLPNetwork is the model class, it extends the **Module** class provided by pytorch. \\\n",
    "- We define \\_\\_init__() function. It sets up layers and defines the model parameters.\n",
    "- Also, we define forward() function which defines how the forwared pass computation occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9b72d79-65b6-4fd0-9d0b-025c2a0fc293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicMLPNetwork(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len, pred_len):\n",
    "        # call the constructor of the base class\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.num_features = num_features\n",
    "        hidden_size_time = 100\n",
    "        hidden_size_feat = 20\n",
    "        # define layers for combining across time series\n",
    "        self.fc1 = torch.nn.Linear(self.seq_len, hidden_size_time)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(hidden_size_time, self.pred_len)\n",
    "\n",
    "        # define layers for combining across the features\n",
    "        self.fc3 = torch.nn.Linear(self.num_features, hidden_size_feat)\n",
    "        self.fc4 = torch.nn.Linear(hidden_size_feat, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # computation over time\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out) # has dimension 512 x 7 x 1\n",
    "\n",
    "        # computation over features\n",
    "        out = out.transpose(1,2) # dimension 512 x 1 x 7\n",
    "        out = self.fc3(out) # dimension 512 x 1 x 20\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out) # dimension 512 x 1 x 1\n",
    "\n",
    "        out = out.squeeze(-1) # dimension 512 x 1\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Note that the gradients are stored insize the FC layer objects\n",
    "# For each training example we need to get rid of these gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5277794b-fe37-4595-8554-d26db5710e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicMLPNetwork(seq_len, pred_len)\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "# there are different optimizer methods, here we're using Adam Optimizer.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1afde643-9953-4129-8415-7b4836c31300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 8])\n",
      "torch.Size([100])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1])\n",
      "torch.Size([20, 7])\n",
      "torch.Size([20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for gen in model.parameters():\n",
    "    print(gen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b3cc7a8-9844-4eae-a601-4787513eb1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape:  torch.Size([512, 7, 8])\n",
      "labels shape:  torch.Size([512, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, (f,l) in enumerate(train_loader):\n",
    "    print('features shape: ', f.shape)\n",
    "    print('labels shape: ', l.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22683e91-642a-494b-abc5-069dc6fa9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics\n",
    "import numpy as np\n",
    "epsilon = np.finfo(float).eps\n",
    "\n",
    "def wape_function(y, y_pred):\n",
    "    \"\"\"Weighted Average Percentage Error metric in the interval [0; 100]\"\"\"\n",
    "    y = np.array(y)\n",
    "    y_pred = np.array(y_pred)\n",
    "    nominator = np.sum(np.abs(np.subtract(y, y_pred)))\n",
    "    denominator = np.add(np.sum(np.abs(y)), epsilon)\n",
    "    wape = np.divide(nominator, denominator) * 100.0\n",
    "    return wape\n",
    "\n",
    "def nse_function(y, y_pred):\n",
    "    y = np.array(y)\n",
    "    y_pred = np.array(y_pred)\n",
    "    return (1-(np.sum((y_pred-y)**2)/np.sum((y-np.mean(y))**2)))\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    # following line prepares the model for evaulation mode. It disables dropout and batch normalization if they have \n",
    "    # are part of the model. For our simple model it's not necessary. Still I'm going to use it.\n",
    "\n",
    "    model.eval()\n",
    "    all_inputs = torch.empty((0, num_features, seq_len))\n",
    "    all_labels = torch.empty(0, pred_len)\n",
    "    for inputs, labels in data_loader:\n",
    "        all_inputs = torch.vstack((all_inputs, inputs))\n",
    "        all_labels = torch.vstack((all_labels, labels))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(all_inputs)\n",
    "        nse = nse_function(all_labels.numpy(), outputs.numpy())\n",
    "        wape = wape_function(all_labels.numpy(), outputs.numpy())\n",
    "        \n",
    "    print(f'NSE : {nse} ', end='')\n",
    "    print(f'WAPE : {wape} ')\n",
    "    \n",
    "    model.train()\n",
    "    return nse, wape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9beaf3a1-2c40-4fc6-b0f7-c352b5763231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 0.016206374802296957 NSE : 0.15714913606643677 WAPE : 39.11094295875523 \n",
      "Epoch 2: 0.013584269159911013 NSE : 0.5186263918876648 WAPE : 27.771585992077323 \n",
      "Epoch 3: 0.004240482592613017 NSE : 0.7875455766916275 WAPE : 20.210333945197505 \n",
      "Epoch 4: 0.0010790594650515286 NSE : 0.9127053394913673 WAPE : 12.171565234218027 \n",
      "Epoch 5: 0.0006093668847422165 NSE : 0.9477205686271191 WAPE : 8.73540323259754 \n",
      "Epoch 6: 0.00044432752840916295 NSE : 0.9574275575578213 WAPE : 7.915840024643277 \n",
      "Epoch 7: 0.00035985031659557055 NSE : 0.9610028378665447 WAPE : 7.763982048978512 \n",
      "Epoch 8: 0.0003255458753563826 NSE : 0.9462717175483704 WAPE : 10.12784190692189 \n",
      "Epoch 9: 0.0003198113809048664 NSE : 0.9152859002351761 WAPE : 13.567782155265734 \n",
      "Epoch 10: 0.0003409921594252013 NSE : 0.8694832772016525 WAPE : 17.483276997324932 \n",
      "Epoch 11: 0.0003368078595784808 NSE : 0.8608902543783188 WAPE : 18.257107194327162 \n",
      "Epoch 12: 0.0003075111486127478 NSE : 0.8762962445616722 WAPE : 17.208524422717716 \n",
      "Epoch 13: 0.00028642785819468086 NSE : 0.8721912801265717 WAPE : 17.547111777854475 \n",
      "Epoch 14: 0.00028509690566242346 NSE : 0.8549609929323196 WAPE : 18.780873634268115 \n",
      "Epoch 15: 0.00031217460755154106 NSE : 0.8149985671043396 WAPE : 21.54417093568096 \n",
      "Epoch 16: 0.00030042585568547863 NSE : 0.8534019887447357 WAPE : 19.136938277928866 \n",
      "Epoch 17: 0.0002564340957422661 NSE : 0.8849536329507828 WAPE : 16.791711952905512 \n",
      "Epoch 18: 0.00024614369791355013 NSE : 0.8632171899080276 WAPE : 18.374761879102213 \n",
      "Epoch 19: 0.00027460343690658215 NSE : 0.8133202195167542 WAPE : 21.726295790005075 \n",
      "Epoch 20: 0.0003078968735805739 NSE : 0.8031145930290222 WAPE : 22.542293456572988 \n",
      "Epoch 21: 0.0002669628301368567 NSE : 0.8804544061422348 WAPE : 17.34639429548284 \n",
      "Epoch 22: 0.00021349078117250214 NSE : 0.9016116932034492 WAPE : 15.531882784742635 \n",
      "Epoch 23: 0.0002168048197214473 NSE : 0.8561661839485168 WAPE : 18.8247004781616 \n",
      "Epoch 24: 0.0002693450172613812 NSE : 0.780555248260498 WAPE : 23.644884294841447 \n",
      "Epoch 25: 0.0003169037576323577 NSE : 0.7919573336839676 WAPE : 23.367042038864646 \n",
      "Epoch 26: 0.00023392883665397164 NSE : 0.9180669486522675 WAPE : 14.229196218164555 \n",
      "Epoch 27: 0.00017913349588616866 NSE : 0.9100873023271561 WAPE : 14.702526950311597 \n",
      "Epoch 28: 0.00019651638334554513 NSE : 0.8553674072027206 WAPE : 18.732195472496 \n",
      "Epoch 29: 0.0002690591425672033 NSE : 0.7579120397567749 WAPE : 24.831030152672255 \n",
      "Epoch 30: 0.0003166329054345124 NSE : 0.8052075058221817 WAPE : 22.71896160090545 \n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = []\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        outputs = model(inputs)\n",
    "        loss_val = loss(outputs, labels)\n",
    "\n",
    "        # calculate gradients for back propagation\n",
    "        loss_val.backward()\n",
    "\n",
    "        # update the weights based on the gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # reset the gradients, avoid gradient accumulation\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss.append(loss_val.item())\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: {sum(epoch_loss)/len(epoch_loss)} ', end='')\n",
    "    nse, wape = evaluate_model(model, val_loader)\n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7216e93-a35d-4aa5-83db-2e43ea380f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : 0.8102979809045792 WAPE : 25.818637390268833 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8102979809045792, 25.818637390268833)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "evaluate_model(model, test_loader)\n",
    "\n",
    "\n",
    "## Write code for plotting the observed values and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b762b299-952d-445e-91bb-bc6bc8720075",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
