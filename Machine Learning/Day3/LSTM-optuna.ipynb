{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ccae883-1904-48a6-81b3-d0228caeab1f",
   "metadata": {},
   "source": [
    "## Multivariate time series prediction using Long Short Term Memory (LSTM)\n",
    "\n",
    "### Theory of Recurrent Neural Network (RNN)\n",
    "\n",
    "The most important difference between RNN and Fully Connected Network(FCN) is that RNN does sequential processing of input data and maintains a hidden state.\n",
    "The hidden state is updated for each sequence input. When we train the model, it learns how to update the hidden state in the best possible way such that it can be used to predict the next value in the sequence. Basic example:\n",
    "\n",
    "### Let's consider a network that learns to count the number of 1's in a binary sequence.\n",
    "\n",
    "So, in an FCN, we feed the entire binary sequence at once, e.g. 1, 0, 1, 1 is fed at once, and it multiplies with a weight matrix of a 4x4 matrix and the computes flows forward.\n",
    "\n",
    "But, in a LSTM, \n",
    "- Model first takes 1 and an initial hidden state h which is normally assigned to be 0, updates the hidden state h,\n",
    "- Then takes 0 and h and updates hidden state h,\n",
    "- Then takes 1 and h updates hidden state h,\n",
    "- Also finally takes 1 and h and updates the hidden state h.\n",
    "\n",
    "  This final h value is used to predict the number of 1's in the binary sequence. Normally this h is passed through a fully connected layer to get the required final output.\n",
    "\n",
    "Mathematically,\n",
    "\n",
    "$$ h_t = W_{xh}x_t + b_t + W_{hh} h_{t-1} + b_h $$\n",
    "\n",
    "Final output is calculated as\n",
    "$$ y_t = h_t W_{hy}^T + b_y $$\n",
    "\n",
    "Here is a PyTorch implementation of an LSTM neural network which has 1 LSTM layer and 1 fully connected layer.\n",
    "\n",
    "\n",
    "### LSTM\n",
    "\n",
    "LSTM is basically an RNN where the way we calculate the hidden state is more advanced and involves multiple equations. In practice, LSTM has been implemented in different time series modeling applications. Pytorch implementation of LSTM uses the following equations in each layer.\n",
    "\n",
    "We initialize the $h_{t-1}$ and $c_{t-1}$ to zeros in every forward pass\n",
    "$$\\begin{array}{ll} \\\\\n",
    "         Input Gate:   i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n",
    "         Forget Gate:   f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
    "         Candidate Cell State:   g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n",
    "         Output Gate:   o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n",
    "         Cell State Update:   c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "         Hidden State:   h_t = o_t \\odot \\tanh(c_t) \\\\\n",
    "        \\end{array}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4117e9d0-7925-4788-bbe1-28c60c80bb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kpanthi/.conda/envs/cenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8af4f85b-3822-4288-9566-0f0ac0cfa053",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'final_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc9eb646-4308-449c-84c0-00cd1de182c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Implement determinism. Set a fixed value for random seed so that when the parameters are initialized, they are initialized same across all experiments.\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# If you are using CUDA, also set the seed for it\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Set the seed for NumPy\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0600e784-ec1c-4ab8-ab6b-9b765b4dee83",
   "metadata": {},
   "source": [
    "Here we define **RiverData** a custom Dataset class to load the dataset we have. It extends the Pytorch Dataset class.  \n",
    "- We need to define \\_\\_init__() function which can be used for loading data from the file and optionally for data preprocessing.\n",
    "- Thereafter we define \\_\\_len__() function which gives the length of dataset.\n",
    "- Then we define \\_\\_getitem__() function which returns an instance of (feature, label) tuple which can be used for model training.\n",
    "  For our time series data, feature means the past values to be used for training and label means the future values to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "742b694d-9966-4fd3-bca7-51f398a8775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiverData(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, df, target, datecol, seq_len, pred_len):\n",
    "        self.df = df\n",
    "        self.datecol = datecol\n",
    "        self.target = target\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.setIndex()\n",
    "        \n",
    "\n",
    "    def setIndex(self):\n",
    "        self.df.set_index(self.datecol, inplace=True)\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) - self.seq_len - self.pred_len\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if len(self.df) <= (idx + self.seq_len+self.pred_len):\n",
    "            raise IndexError(f\"Index {idx} is out of bounds for dataset of size {len(self.df)}\")\n",
    "        df_piece = self.df[idx:idx+self.seq_len].values\n",
    "        feature = torch.tensor(df_piece, dtype=torch.float32)\n",
    "        label_piece = self.df[self.target][idx + self.seq_len:  idx+self.seq_len+self.pred_len].values\n",
    "        label = torch.tensor(label_piece, dtype=torch.float32)\n",
    "        return (feature, label) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c24959-e72f-435d-8a14-69a648fa5ab1",
   "metadata": {},
   "source": [
    "### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d897d9-946e-4bbe-90a6-cc8d8a223c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)\n",
    "raw_df = df.drop('DATE', axis=1, inplace=False)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply the transformations\n",
    "df_scaled = scaler.fit_transform(raw_df)\n",
    "\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=raw_df.columns)\n",
    "df_scaled['DATE'] = df['DATE']\n",
    "df = df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4781c6-88ed-4b28-aa0e-f669bf7c75f5",
   "metadata": {},
   "source": [
    "Some advanced Python syntax has been used here. \\\n",
    "*common_args : it's used to pass arguments to a function, where common_args represents a python list \\\n",
    "**common_args: it's used to pass arguments to a function, where common_args represents a python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e63a535-a817-4429-be17-f3e3a02e4f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_size = int(0.7 * len(df))\n",
    "test_size = int(0.2 * len(df))\n",
    "val_size = len(df) - train_size - test_size\n",
    "\n",
    "seq_len = 13\n",
    "pred_len = 1\n",
    "num_features = 7\n",
    "\n",
    "common_args = ['gauge_height', 'DATE', seq_len, pred_len]\n",
    "train_dataset = RiverData(df[:train_size], *common_args)\n",
    "val_dataset = RiverData(df[train_size: train_size+val_size], *common_args)\n",
    "test_dataset = RiverData(df[train_size+val_size : len(df)], *common_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "501879b1-dd1c-4ff0-afb3-ff25a345df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important parameters\n",
    "\n",
    "BATCH_SIZE = 512 # keep as big as can be handled by GPU and memory\n",
    "SHUFFLE = False # we don't shuffle the time series data\n",
    "DATA_LOAD_WORKERS = 1 # it depends on the amount of data you need to load\n",
    "learning_rate = 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae359aec-9435-4229-8587-5f120b0370b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "common_args = {'batch_size': BATCH_SIZE, 'shuffle': SHUFFLE}\n",
    "train_loader = DataLoader(train_dataset, **common_args)\n",
    "val_loader = DataLoader(val_dataset, **common_args)\n",
    "test_loader = DataLoader(test_dataset, **common_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ac35c-8ecc-4ab7-a97c-0cb4d59bc624",
   "metadata": {},
   "source": [
    "### Here we define our PyTorch model.\n",
    "\n",
    "BasicLSTMNetwork is the model class, it extends the Module class provided by pytorch. \\\n",
    "- We define \\_\\_init__() function. It sets up layers and defines the model parameters.\n",
    "- Also, we define forward() function which defines how the forwared pass computation occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01cd8d0d-2058-4dfe-9046-78cde5fd2f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLSTMNetwork(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len, pred_len):\n",
    "        # call the constructor of the base class\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.num_features = num_features\n",
    "        self.n_layers = 1\n",
    "        \n",
    "        self.n_hidden = 128\n",
    "        \n",
    "        # define layers for combining across time series\n",
    "        self.lstm1 = torch.nn.LSTM(input_size = self.num_features, hidden_size = self.n_hidden, num_layers=self.n_layers, batch_first = True)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc1 = torch.nn.Linear(self.n_hidden * self.seq_len, self.pred_len)\n",
    "\n",
    "\n",
    "    def init_hidden(self, batchsize):\n",
    "        device = next(self.parameters()).device\n",
    "        hidden_state = torch.zeros(self.n_layers, batchsize, self.n_hidden, device=device)\n",
    "        cell_state = torch.zeros(self.n_layers, batchsize, self.n_hidden, device=device)\n",
    "        return hidden_state, cell_state\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batchsize, seqlen, featlen = x.size()\n",
    "        self.hidden_states = self.init_hidden(batchsize)\n",
    "        lstm_out, self.hidden_states = self.lstm1(x, self.hidden_states)\n",
    "        lstm_out = lstm_out.contiguous().view(batchsize, -1)\n",
    "        lstm_out = self.relu(lstm_out)\n",
    "        lstm_out = self.fc1(lstm_out)\n",
    "        return lstm_out\n",
    "# Note that the gradients are stored insize the FC layer objects\n",
    "# For each training example we need to get rid of these gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f579dc18-584e-414b-ae8e-37f62e4b42f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b600c877-3409-48f9-80e5-3fdde7731817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5277794b-fe37-4595-8554-d26db5710e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afde643-9953-4129-8415-7b4836c31300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b3cc7a8-9844-4eae-a601-4787513eb1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape:  torch.Size([512, 13, 7])\n",
      "labels shape:  torch.Size([512, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, (f,l) in enumerate(train_loader):\n",
    "    print('features shape: ', f.shape)\n",
    "    print('labels shape: ', l.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22683e91-642a-494b-abc5-069dc6fa9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics\n",
    "import numpy as np\n",
    "epsilon = np.finfo(float).eps\n",
    "\n",
    "def wape_function(y, y_pred):\n",
    "    \"\"\"Weighted Average Percentage Error metric in the interval [0; 100]\"\"\"\n",
    "    y = np.array(y)\n",
    "    y_pred = np.array(y_pred)\n",
    "    nominator = np.sum(np.abs(np.subtract(y, y_pred)))\n",
    "    denominator = np.add(np.sum(np.abs(y)), epsilon)\n",
    "    wape = np.divide(nominator, denominator) * 100.0\n",
    "    return wape\n",
    "\n",
    "def nse_function(y, y_pred):\n",
    "    y = np.array(y)\n",
    "    y_pred = np.array(y_pred)\n",
    "    return (1-(np.sum((y_pred-y)**2)/np.sum((y-np.mean(y))**2)))\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    # following line prepares the model for evaluation mode. It disables dropout and batch normalization if they have \n",
    "    # are part of the model. For our simple model, it's not necessary. Still, I'm going to use it.\n",
    "\n",
    "    model.eval()\n",
    "    all_inputs = torch.empty((0, seq_len, num_features))\n",
    "    all_labels = torch.empty(0, pred_len)\n",
    "    for inputs, labels in data_loader:\n",
    "        all_inputs = torch.vstack((all_inputs, inputs))\n",
    "        all_labels = torch.vstack((all_labels, labels))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        all_inputs = all_inputs.to(device)\n",
    "        outputs = model(all_inputs).detach().cpu()\n",
    "        avg_val_loss = loss(outputs, all_labels)\n",
    "        nse = nse_function(all_labels.numpy(), outputs.numpy())\n",
    "        wape = wape_function(all_labels.numpy(), outputs.numpy())\n",
    "        \n",
    "    print(f'NSE : {nse}', end=' ')\n",
    "    print(f'WAPE : {wape}', end=' ')\n",
    "    print(f'Validation Loss: {avg_val_loss}')\n",
    "    model.train()\n",
    "    return avg_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9beaf3a1-2c40-4fc6-b0f7-c352b5763231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:04:24,044] A new study created in memory with name: no-name-757d3e92-2f76-4796-9da6-0c87a6cc172f\n",
      "/local_scratch/slurm.1125506/ipykernel_3252748/2556302494.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
      "/local_scratch/slurm.1125506/ipykernel_3252748/2556302494.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Traning Loss: 0.014093939006564369 NSE : -0.6168206930160522 WAPE : 77.44650288799536 Validation Loss: 0.027603207156062126\n",
      "Epoch 2: Traning Loss: 0.01087123653708885 NSE : 0.0015901923179626465 WAPE : 60.48224610044913 Validation Loss: 0.01704537309706211\n",
      "Epoch 3: Traning Loss: 0.007364156993320519 NSE : 0.5486051440238953 WAPE : 39.16127082360514 Validation Loss: 0.007706447504460812\n",
      "Epoch 4: Traning Loss: 0.004622035719534657 NSE : 0.7580589205026627 WAPE : 27.21856395390195 Validation Loss: 0.004130543675273657\n",
      "Epoch 5: Traning Loss: 0.0033482389509362324 NSE : 0.797127902507782 WAPE : 24.704020972741354 Validation Loss: 0.0034635381307452917\n",
      "Epoch 6: Traning Loss: 0.002861688583516067 NSE : 0.8124763369560242 WAPE : 23.735947389957904 Validation Loss: 0.003201501676812768\n",
      "Epoch 7: Traning Loss: 0.0026119323752789173 NSE : 0.8206541240215302 WAPE : 23.199395989556592 Validation Loss: 0.003061886178329587\n",
      "Epoch 8: Traning Loss: 0.0024772764033703537 NSE : 0.8248678296804428 WAPE : 22.93931001526225 Validation Loss: 0.0029899480286985636\n",
      "Epoch 9: Traning Loss: 0.002394462889643932 NSE : 0.8288987576961517 WAPE : 22.64882455313721 Validation Loss: 0.002921129111200571\n",
      "Epoch 10: Traning Loss: 0.0023339879138027033 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:05:43,663] Trial 0 finished with value: 0.002907732268795371 and parameters: {'lr': 0.0007561625800237071, 'weight_decay': 0.004708536115006036}. Best is trial 0 with value: 0.002907732268795371.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : 0.8296834826469421 WAPE : 22.595315115826185 Validation Loss: 0.002907732268795371\n",
      "Epoch 1: Traning Loss: 0.006959353710053758 NSE : 0.7103018164634705 WAPE : 28.061775580116954 Validation Loss: 0.004945878405123949\n",
      "Epoch 2: Traning Loss: 0.002870339009707538 NSE : 0.7805216610431671 WAPE : 21.699795769673123 Validation Loss: 0.003747048554942012\n",
      "Epoch 3: Traning Loss: 0.00239318782798807 NSE : 0.872596874833107 WAPE : 18.43789683084679 Validation Loss: 0.0021750922314822674\n",
      "Epoch 4: Traning Loss: 0.0021330302906097656 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:06:15,384] Trial 1 finished with value: 0.0021750922314822674 and parameters: {'lr': 0.00739110623107165, 'weight_decay': 0.0032251005173378213}. Best is trial 1 with value: 0.0021750922314822674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : 0.6995921432971954 WAPE : 26.81292491493778 Validation Loss: 0.005128719378262758\n",
      "Early stopping!\n",
      "Epoch 1: Traning Loss: 0.012103991124456627 NSE : -0.2770369052886963 WAPE : 68.79279962643757 Validation Loss: 0.02180224098265171\n",
      "Epoch 2: Traning Loss: 0.006492827798468581 NSE : 0.8003312200307846 WAPE : 23.711842612176135 Validation Loss: 0.003408849472180009\n",
      "Epoch 3: Traning Loss: 0.0025353606544768316 NSE : 0.8427001237869263 WAPE : 20.512998620269986 Validation Loss: 0.002685505198314786\n",
      "Epoch 4: Traning Loss: 0.0016571320549036532 NSE : 0.8844922110438347 WAPE : 17.93587169496909 Validation Loss: 0.0019720089621841908\n",
      "Epoch 5: Traning Loss: 0.001184242763399202 NSE : 0.8966521397233009 WAPE : 17.123673620956684 Validation Loss: 0.0017644085455685854\n",
      "Epoch 6: Traning Loss: 0.0010054516390339485 NSE : 0.9079659432172775 WAPE : 16.04966843165054 Validation Loss: 0.0015712531749159098\n",
      "Epoch 7: Traning Loss: 0.0009132620324436836 NSE : 0.9171340689063072 WAPE : 15.18050433216912 Validation Loss: 0.0014147303299978375\n",
      "Epoch 8: Traning Loss: 0.0008324948212986949 NSE : 0.9264554530382156 WAPE : 14.234194819427218 Validation Loss: 0.0012555908178910613\n",
      "Epoch 9: Traning Loss: 0.0007630525782366336 NSE : 0.9353247806429863 WAPE : 13.233688073258804 Validation Loss: 0.0011041690595448017\n",
      "Epoch 10: Traning Loss: 0.0007055499101929877 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:07:34,868] Trial 2 finished with value: 0.0009632355067878962 and parameters: {'lr': 0.0003934442587122108, 'weight_decay': 0.0001942244840788907}. Best is trial 2 with value: 0.0009632355067878962.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : 0.9435797743499279 WAPE : 12.194063885936684 Validation Loss: 0.0009632355067878962\n",
      "Epoch 1: Traning Loss: 0.011557344935667763 NSE : 0.198744535446167 WAPE : 53.30042212413626 Validation Loss: 0.013679451309144497\n",
      "Epoch 2: Traning Loss: 0.007646651394005752 NSE : 0.7858569920063019 WAPE : 24.85425545289162 Validation Loss: 0.0036559610161930323\n",
      "Epoch 3: Traning Loss: 0.0021842981974989043 NSE : 0.8158048689365387 WAPE : 22.786797554377493 Validation Loss: 0.0031446751672774553\n",
      "Epoch 4: Traning Loss: 0.0013912819028015929 NSE : 0.9160984382033348 WAPE : 14.819676585721098 Validation Loss: 0.001432411139830947\n",
      "Epoch 5: Traning Loss: 0.0010805342988043712 NSE : 0.936137244105339 WAPE : 12.598536422688989 Validation Loss: 0.0010902982903644443\n",
      "Epoch 6: Traning Loss: 0.0009661771159722212 NSE : 0.947092991322279 WAPE : 11.26966590672349 Validation Loss: 0.000903255888260901\n",
      "Epoch 7: Traning Loss: 0.0008649240202624422 NSE : 0.9538114666938782 WAPE : 10.436657873440938 Validation Loss: 0.0007885547238402069\n",
      "Epoch 8: Traning Loss: 0.0007827825117837059 NSE : 0.9583994075655937 WAPE : 9.843040304456196 Validation Loss: 0.0007102270610630512\n",
      "Epoch 9: Traning Loss: 0.0007189529770321915 NSE : 0.9607907310128212 WAPE : 9.59394476185043 Validation Loss: 0.0006694011390209198\n",
      "Epoch 10: Traning Loss: 0.0006652169841029066 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:08:54,357] Trial 3 finished with value: 0.0006196001777425408 and parameters: {'lr': 0.0010292516182143097, 'weight_decay': 0.0002503903940662566}. Best is trial 3 with value: 0.0006196001777425408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : 0.9637077525258064 WAPE : 9.170560032869831 Validation Loss: 0.0006196001777425408\n",
      "Epoch 1: Traning Loss: 0.013700634541819084 NSE : -0.755825400352478 WAPE : 80.86985673768156 Validation Loss: 0.029976362362504005\n",
      "Epoch 2: Traning Loss: 0.01198190504000735 NSE : -0.213647723197937 WAPE : 66.98103240992138 Validation Loss: 0.020720025524497032\n",
      "Epoch 3: Traning Loss: 0.009417215886060148 NSE : 0.3084389567375183 WAPE : 49.795928141350835 Validation Loss: 0.011806690134108067\n",
      "Epoch 4: Traning Loss: 0.006673192336128878 NSE : 0.5082704424858093 WAPE : 41.2629072369269 Validation Loss: 0.008395062759518623\n",
      "Epoch 5: Traning Loss: 0.005022590549211821 NSE : 0.6248090863227844 WAPE : 35.63385177093524 Validation Loss: 0.006405454128980637\n",
      "Epoch 6: Traning Loss: 0.004451752275105585 NSE : 0.6763741970062256 WAPE : 32.95126424189153 Validation Loss: 0.005525107961148024\n",
      "Epoch 7: Traning Loss: 0.003996230542425156 NSE : 0.7014378607273102 WAPE : 31.57643133783101 Validation Loss: 0.0050972080789506435\n",
      "Epoch 8: Traning Loss: 0.0037237601448899386 NSE : 0.7324641048908234 WAPE : 29.631123828479627 Validation Loss: 0.004567511845380068\n",
      "Epoch 9: Traning Loss: 0.0034732367625627857 NSE : 0.7521808743476868 WAPE : 28.299356865512593 Validation Loss: 0.004230896942317486\n",
      "Epoch 10: Traning Loss: 0.003254878090273206 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:10:13,743] Trial 4 finished with value: 0.004047873895615339 and parameters: {'lr': 0.0010561717817610934, 'weight_decay': 0.007798537139907157}. Best is trial 3 with value: 0.0006196001777425408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : 0.7629012167453766 WAPE : 27.578377063520403 Validation Loss: 0.004047873895615339\n",
      "Epoch 1: Traning Loss: 0.011552640947224904 NSE : -0.062052369117736816 WAPE : 62.57483661926002 Validation Loss: 0.018131909891963005\n",
      "Epoch 2: Traning Loss: 0.0061433927150417195 NSE : 0.8206175118684769 WAPE : 21.771738142879176 Validation Loss: 0.0030625115614384413\n",
      "Epoch 3: Traning Loss: 0.0020877651327028445 NSE : 0.8546998798847198 WAPE : 19.985264215832313 Validation Loss: 0.0024806393776088953\n",
      "Epoch 4: Traning Loss: 0.0013990596782263642 NSE : 0.8979388698935509 WAPE : 16.84246515283732 Validation Loss: 0.0017424407415091991\n",
      "Epoch 5: Traning Loss: 0.0010515293974380912 NSE : 0.9222323149442673 WAPE : 14.46097825604662 Validation Loss: 0.0013276904355734587\n",
      "Epoch 6: Traning Loss: 0.0009202024969818935 NSE : 0.9339597225189209 WAPE : 13.223451466112804 Validation Loss: 0.0011274740099906921\n",
      "Epoch 7: Traning Loss: 0.0008194794926599145 NSE : 0.9437585547566414 WAPE : 12.053527881251659 Validation Loss: 0.0009601833298802376\n",
      "Epoch 8: Traning Loss: 0.0007411015579093728 NSE : 0.9521603174507618 WAPE : 10.883434823427661 Validation Loss: 0.0008167439955286682\n",
      "Epoch 9: Traning Loss: 0.000680838573648982 NSE : 0.9582793153822422 WAPE : 9.91250220975136 Validation Loss: 0.0007122772512957454\n",
      "Epoch 10: Traning Loss: 0.00062955096926523 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:11:32,406] Trial 5 finished with value: 0.0006474709371104836 and parameters: {'lr': 0.0005584104884190766, 'weight_decay': 0.00016854828502381434}. Best is trial 3 with value: 0.0006196001777425408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : 0.9620752595365047 WAPE : 9.28503626809443 Validation Loss: 0.0006474709371104836\n",
      "Epoch 1: Traning Loss: 0.012081803504964407 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:11:40,249] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.3618673086166382 WAPE : 71.07080237040293 Validation Loss: 0.023250507190823555\n",
      "Epoch 1: Traning Loss: 0.004886623936159262 NSE : 0.8436838239431381 WAPE : 21.317251273556302 Validation Loss: 0.002668710658326745\n",
      "Epoch 2: Traning Loss: 0.0016762010899708053 NSE : 0.9178655594587326 WAPE : 15.099139702417801 Validation Loss: 0.0014022418763488531\n",
      "Epoch 3: Traning Loss: 0.0012021332059624643 NSE : 0.9306038469076157 WAPE : 13.692442579919048 Validation Loss: 0.001184767228551209\n",
      "Epoch 4: Traning Loss: 0.0010236001697859105 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:12:11,750] Trial 7 finished with value: 0.001184767228551209 and parameters: {'lr': 0.005057059582258928, 'weight_decay': 0.0010239036710077514}. Best is trial 3 with value: 0.0006196001777425408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : 0.9285470694303513 WAPE : 13.362277334536282 Validation Loss: 0.0012198816984891891\n",
      "Early stopping!\n",
      "Epoch 1: Traning Loss: 0.013603427543849708 NSE : 0.08792859315872192 WAPE : 56.866956417012304 Validation Loss: 0.015571357682347298\n",
      "Epoch 2: Traning Loss: 0.010729435714302103 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:12:27,536] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : 0.21875756978988647 WAPE : 53.104159227463875 Validation Loss: 0.013337777927517891\n",
      "Epoch 1: Traning Loss: 0.012761181334168342 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:12:35,444] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.4499669075012207 WAPE : 73.46090817787601 Validation Loss: 0.024754589423537254\n",
      "Epoch 1: Traning Loss: 0.010124910320199644 NSE : 0.2732051610946655 WAPE : 49.88540986573868 Validation Loss: 0.012408220209181309\n",
      "Epoch 2: Traning Loss: 0.0036947255500820986 NSE : 0.8669676333665848 WAPE : 18.554874993221084 Validation Loss: 0.0022711975034326315\n",
      "Epoch 3: Traning Loss: 0.001481130956927714 NSE : 0.939994465559721 WAPE : 12.148024464676729 Validation Loss: 0.0010244457516819239\n",
      "Epoch 4: Traning Loss: 0.0010678783823058794 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:13:06,860] Trial 10 finished with value: 0.0010244457516819239 and parameters: {'lr': 0.0018044440036282699, 'weight_decay': 3.2119745657494695e-05}. Best is trial 3 with value: 0.0006196001777425408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : 0.9396714940667152 WAPE : 12.900838772314968 Validation Loss: 0.001029959530569613\n",
      "Early stopping!\n",
      "Epoch 1: Traning Loss: 0.01332901565264674 NSE : 0.24784469604492188 WAPE : 48.4659043288589 Validation Loss: 0.012841186486184597\n",
      "Epoch 2: Traning Loss: 0.010516184145173408 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:13:22,631] Trial 11 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : 0.3513110876083374 WAPE : 47.97707387366068 Validation Loss: 0.011074754409492016\n",
      "Epoch 1: Traning Loss: 0.007109248048187408 NSE : 0.7098385691642761 WAPE : 27.23824042517377 Validation Loss: 0.004953786730766296\n",
      "Epoch 2: Traning Loss: 0.0036807611907021553 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:13:38,332] Trial 12 finished with value: 0.004953786730766296 and parameters: {'lr': 0.002420484822582271, 'weight_decay': 8.303816072011545e-05}. Best is trial 3 with value: 0.0006196001777425408.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : 0.6432669162750244 WAPE : 31.595275850921546 Validation Loss: 0.006090333219617605\n",
      "Early stopping!\n",
      "Epoch 1: Traning Loss: 0.012176520158483547 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:13:46,198] Trial 13 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : 0.12205725908279419 WAPE : 56.64490721861502 Validation Loss: 0.014988696202635765\n",
      "Epoch 1: Traning Loss: 0.009830789359258062 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:13:54,060] Trial 14 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.02518165111541748 WAPE : 47.28447088868276 Validation Loss: 0.01750243455171585\n",
      "Epoch 1: Traning Loss: 0.011876849364289874 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:14:01,927] Trial 15 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.10741662979125977 WAPE : 63.80023618808558 Validation Loss: 0.01890639401972294\n",
      "Epoch 1: Traning Loss: 0.013842971368780274 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:14:09,811] Trial 16 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.16296052932739258 WAPE : 65.15491501525169 Validation Loss: 0.01985466480255127\n",
      "Epoch 1: Traning Loss: 0.00887467448028197 NSE : 0.24350106716156006 WAPE : 49.93456142686198 Validation Loss: 0.012915343046188354\n",
      "Epoch 2: Traning Loss: 0.004444658466399681 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:14:25,499] Trial 17 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : 0.5608006417751312 WAPE : 35.934107239321534 Validation Loss: 0.007498240098357201\n",
      "Epoch 1: Traning Loss: 0.011952241111799678 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:14:33,331] Trial 18 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.2190399169921875 WAPE : 67.2187743644896 Validation Loss: 0.020812084898352623\n",
      "Epoch 1: Traning Loss: 0.010832582341213379 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 16:14:41,063] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE : -0.021014690399169922 WAPE : 61.451041594035395 Validation Loss: 0.017431294545531273\n",
      "Number of finished trials: 20\n",
      "Best trial:\n",
      "  Value (Best Validation Loss): 0.0006196001777425408\n",
      "  Params:\n",
      "    lr: 0.0010292516182143097\n",
      "    weight_decay: 0.0002503903940662566\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Here we define the search space of the hyper-parameters. Optuna uses byaesian optimization to find the optimal values of the hyperparameters.\n",
    "    learning_rate = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n",
    "\n",
    "    model = BasicLSTMNetwork(seq_len, pred_len)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # I have kept it low for the demonstration purpose. In real world scenario it should be kept high ~ 50.\n",
    "    num_epochs = 10\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Same for this, I have kept it low for demonstration purpose. In real world scenario it should be high ~ 5 to 10.\n",
    "    patience = 1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = []\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss_val = loss(outputs, labels)\n",
    "    \n",
    "            # calculate gradients for back propagation\n",
    "            loss_val.backward()\n",
    "    \n",
    "            # update the weights based on the gradients\n",
    "            optimizer.step()\n",
    "    \n",
    "            # reset the gradients, avoid gradient accumulation\n",
    "            optimizer.zero_grad()\n",
    "            epoch_loss.append(loss_val.item())\n",
    "    \n",
    "        avg_train_loss = sum(epoch_loss)/len(epoch_loss)\n",
    "        print(f'Epoch {epoch+1}: Traning Loss: {avg_train_loss}', end=' ')\n",
    "        avg_val_loss = evaluate_model(model, val_loader)\n",
    "    \n",
    "        # Check for improvement\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), 'best_model_trial.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print('Early stopping!')\n",
    "                # Load the best model before stopping\n",
    "                model.load_state_dict(torch.load('best_model_trial.pth'))\n",
    "                break\n",
    "\n",
    "        # Report intermediate objective value\n",
    "        trial.report(best_val_loss, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "print('  Value (Best Validation Loss):', trial.value)\n",
    "print('  Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7216e93-a35d-4aa5-83db-2e43ea380f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "\n",
    "# Optimization history\n",
    "fig1 = vis.plot_optimization_history(study)\n",
    "fig1.write_html(\"optimization_history_lstm.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722fee71-de14-463f-abf6-e35f9f002ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2f592f-d2be-4ea2-9e9e-d034d45f551d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
