{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37b78571-160e-43b4-a8c0-6317bde9c3e8",
   "metadata": {},
   "source": [
    "### Bias Variance Tradeoff.\n",
    "\n",
    "**Our objective is to achieve low bias and low variance.**\n",
    "\n",
    "### Bias\n",
    "\n",
    "**Bias** refers to errors that come from incorrect assumptions in the learning algorithm. It's like having a preconceived notion that all fruits are round and sweet.\n",
    "\n",
    "High Bias: The model makes strong assumptions about the data, leading to systematic errors. It's like always aiming your arrows towards the same spot, regardless of where the target is.\n",
    "\n",
    "Example: If you're using a simple model to predict house prices, it might only consider the size and ignore other important factors like location. This oversimplification leads to inaccurate predictions.\n",
    "Low Bias: The model makes fewer assumptions and can capture more complex patterns. High bias is also known as underfitting.\n",
    "\n",
    "### Variance\n",
    "**Variance** refers to how much the model's predictions fluctuate for different sets of training data. Example, we have cat images and we have clustered them into 3 sets. And we train models independently using these 3 different sets. If the features learned by the model differ greatly, then it's said to have high variance. It means the model has overfitted to the training data and is sensitive to input data.\n",
    "\n",
    "Low Variance: The model's predictions are more consistent across different datasets. High variance is also known as overfitting.\n",
    "\n",
    "In practice when you train models, when you want to decrease bias, there's high change of having high variance and when you try to reduce variance, there's high chance of having high bias. So, they are studied together as bias vs variance tradeoff. We need to be aware of these issues when training the machine learning models. Usually larger models and lot of training can lead to high variance and smaller models and not enough training leads to high bias. This is often one of the biggest issues in ML.\n",
    "\n",
    "**We normally use regularization to reduce/avoid overfitting when training models**\n",
    "\n",
    "In this notebook, we will use different regression techniques, linear, polynomial, Lasso and Ridge regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88b01ade-21c0-4ed1-bd88-3e5d2f61c14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visual style\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6936c912-5f51-48e7-bff8-800213a11b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ask evapotransipration data with Mostafa, change dataset and work again.\n",
    "'''\n",
    "\n",
    "df = pd.read_csv('../dataset/water_potability.csv')\n",
    "df = df.drop(['Potability'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24de1282-7e19-4f19-ab34-98bc67829b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ph 491\n",
      "Hardness 0\n",
      "Solids 0\n",
      "Chloramines 0\n",
      "Sulfate 781\n",
      "Conductivity 0\n",
      "Organic_carbon 0\n",
      "Trihalomethanes 162\n",
      "Turbidity 0\n"
     ]
    }
   ],
   "source": [
    "len(df)\n",
    "\n",
    "for col in df.columns:\n",
    "    print(col, df[col].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c27c7850-ebff-4cd7-9356-7a66fa5ea132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the water quality dataset from a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame): Loaded DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"\n",
    "    Handles missing values in the dataset by removing the columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to process.\n",
    "\n",
    "    Returns:\n",
    "    - df_clean (pd.DataFrame): DataFrame with missing values handled.\n",
    "    \"\"\"\n",
    "    # Check for missing values\n",
    "    missing = df.isnull().sum()\n",
    "    print(\"Missing Values in Each Column:\")\n",
    "    print(missing)\n",
    "    \n",
    "    # drop the columns with nan values\n",
    "    df_clean = df.dropna()\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def split_dataset(df, target_column, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to split.\n",
    "    - target_column (str): The name of the target column.\n",
    "    - test_size (float): Proportion of the dataset to include in the test split.\n",
    "    - random_state (int): Controls the shuffling applied to the data before splitting.\n",
    "\n",
    "    Returns:\n",
    "    - X_train, X_test, y_train, y_test: Split datasets.\n",
    "    \"\"\"\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "def scale_features(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Scales the features using StandardScaler.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame or np.ndarray): Training feature set.\n",
    "    - X_test (pd.DataFrame or np.ndarray): Testing feature set.\n",
    "\n",
    "    Returns:\n",
    "    - X_train_scaled, X_test_scaled (np.ndarray): Scaled feature sets.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "\n",
    "\n",
    "def multiple_linear_regression(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Trains a Multiple Linear Regression model.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (np.ndarray): Scaled training features.\n",
    "    - y_train (pd.Series or np.ndarray): Training target.\n",
    "\n",
    "    Returns:\n",
    "    - model (LinearRegression): Trained Linear Regression model.\n",
    "    \"\"\"\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def polynomial_regression(X_train, y_train, degree=2):\n",
    "    \"\"\"\n",
    "    Trains a Polynomial Regression model of a specified degree.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (np.ndarray): Scaled training features.\n",
    "    - y_train (pd.Series or np.ndarray): Training target.\n",
    "    - degree (int): Degree of the polynomial.\n",
    "\n",
    "    Returns:\n",
    "    - poly_model (Pipeline): Trained Polynomial Regression pipeline.\n",
    "    \"\"\"\n",
    "    poly_features = PolynomialFeatures(degree=degree)\n",
    "    lin_reg = LinearRegression()\n",
    "    poly_model = Pipeline([\n",
    "        ('poly_features', poly_features),\n",
    "        ('linear_regression', lin_reg)\n",
    "    ])\n",
    "    poly_model.fit(X_train, y_train)\n",
    "    return poly_model\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name='Model'):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set and prints performance metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained model with a predict method.\n",
    "    - X_test (np.ndarray): Scaled testing features.\n",
    "    - y_test (pd.Series or np.ndarray): Testing target.\n",
    "    - model_name (str): Name of the model for display purposes.\n",
    "\n",
    "    Returns:\n",
    "    - metrics (dict): Dictionary containing MSE, MAE, and R² Score.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"{model_name} Performance:\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\\n\")\n",
    "    \n",
    "    metrics = {'MSE': mse, 'MAE': mae, 'R²': r2}\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7629e663-d0a9-4d0e-a463-1e38172fabac",
   "metadata": {},
   "source": [
    "### L1 Regularization\n",
    "\n",
    "#### Problem Setup:\n",
    "Imagine you are trying to fit a line (or another model) to a set of data points. The goal is to minimize the difference between the predicted and actual values—this difference is called **error**.\n",
    "\n",
    "However, when you're trying to fit a line too perfectly (reducing the error to almost zero), you might end up with a complicated line that doesn’t generalize well to new data. This is called **overfitting**. To avoid overfitting, we add regularization, which penalizes the complexity of the model.\n",
    "\n",
    "#### What is L1 Regularization?\n",
    "\n",
    "L1 regularization works by adding a penalty term to your cost function (the function you want to minimize). The penalty is based on the absolute values of the model’s coefficients (the numbers that define the line or model). The purpose is to encourage some of the coefficients to become smaller or even zero, which simplifies the model.\n",
    "\n",
    "#### Basic Concept:\n",
    "Let’s say your model has some coefficients $ w_1, w_2, \\dots, w_n $. Normally, you would minimize the error term (let’s call it **E**) that measures how far off your model’s predictions are from the actual data.\n",
    "\n",
    "The error term might look like:\n",
    "$$\n",
    "\\text{Error} = E(w_1, w_2, \\dots, w_n)\n",
    "$$\n",
    "\n",
    "In L1 regularization, you add an additional term to the error:\n",
    "$$\n",
    "\\text{Total Cost} = E(w_1, w_2, \\dots, w_n) + \\lambda \\sum_{i=1}^n |w_i|\n",
    "$$\n",
    "Where:\n",
    "- $ \\sum_{i=1}^n |w_i| $ is the sum of the absolute values of the model’s coefficients.\n",
    "- $ \\lambda $ is a constant that controls how strong the penalty is. If $ \\lambda $ is large, the penalty becomes stronger, and the coefficients get smaller.\n",
    "\n",
    "\n",
    "1. **Absolute values**: The regularization term involves absolute values, which you might recognize from high school. Absolute values just make sure everything is positive, so $ |w_i| $ is always a non-negative number. For example, $ |3| = 3 $ and $ |-3| = 3 $.\n",
    "\n",
    "2. **Summing up coefficients**: You are adding up the absolute values of all the coefficients of your model (like the slope and intercept in a line equation).\n",
    "\n",
    "3. **Penalizing complexity**: This added term penalizes large coefficients. If a coefficient becomes too large, it increases the total cost, making it harder for the model to choose complex solutions. In fact, some coefficients may shrink to exactly zero, which simplifies the model by removing unnecessary features.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Imagine you have a simple linear model:\n",
    "$$\n",
    "y = w_1 x_1 + w_2 x_2 + b\n",
    "$$\n",
    "Normally, you’d just try to minimize the error, but with L1 regularization, you also add the penalty term:\n",
    "$$\n",
    "\\text{Cost} = \\text{Error} + \\lambda (|w_1| + |w_2|)\n",
    "$$\n",
    "If $ w_1 $ or $ w_2 $ gets too large, the total cost will increase because of the absolute values, and the model will prefer to keep these coefficients smaller. In some cases, it might even set $ w_1 $ or $ w_2 $ to zero if it doesn't significantly improve the predictions.\n",
    "\n",
    "#### Why is it useful?\n",
    "By forcing some of the coefficients to zero, L1 regularization helps make the model simpler and less prone to overfitting. This is especially useful when you have many variables, and some of them aren’t very important.\n",
    "\n",
    "#### Summary:\n",
    "- **L1 regularization** adds a penalty based on the sum of the absolute values of the model’s coefficients.\n",
    "- It **simplifies the model** by encouraging smaller or zero coefficients.\n",
    "- The penalty term discourages the model from overfitting to the training data, making it more generalizable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c13c0331-34de-4cdd-9573-83ce83edcc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_regression(X_train, y_train, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Trains a Lasso Regression model with L1 regularization.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (np.ndarray): Scaled training features.\n",
    "    - y_train (pd.Series or np.ndarray): Training target.\n",
    "    - alpha (float): Regularization strength.\n",
    "\n",
    "    Returns:\n",
    "    - lasso_model (Lasso): Trained Lasso Regression model.\n",
    "    \"\"\"\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    return lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fae22a9-70ec-493c-a0f7-6e12ab3e2b55",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "Ridge regression, like L1 regularization, is used to prevent overfitting in models by adding a penalty term. The key difference is that Ridge regression uses the sum of the **squares** of the coefficients (instead of their absolute values) as the penalty.\n",
    "\n",
    "#### Problem Setup:\n",
    "As before, let’s assume you are trying to fit a line (or another model) to a set of data points. The goal is to minimize the **error** (the difference between the predicted values and the actual values).\n",
    "\n",
    "If you fit the model too perfectly, it could lead to **overfitting**—where the model captures noise or irrelevant patterns in the data, making it perform poorly on new data. To avoid this, Ridge regression adds a penalty term that discourages large coefficients and helps to simplify the model.\n",
    "\n",
    "#### What is Ridge Regression?\n",
    "\n",
    "In Ridge regression, the penalty term is based on the **squared values** of the model’s coefficients. The more complex (larger) the coefficients, the larger the penalty becomes.\n",
    "\n",
    "#### Basic Concept:\n",
    "Let’s say your model has coefficients $ w_1, w_2, \\dots, w_n $. Normally, you would minimize the error term **E**, which measures how far off the predictions are from the actual data.\n",
    "\n",
    "The error term might look like:\n",
    "$$\n",
    "\\text{Error} = E(w_1, w_2, \\dots, w_n)\n",
    "$$\n",
    "\n",
    "In Ridge regression, you add a penalty based on the sum of the squares of the coefficients:\n",
    "$$\n",
    "\\text{Total Cost} = E(w_1, w_2, \\dots, w_n) + \\lambda \\sum_{i=1}^n w_i^2\n",
    "$$\n",
    "Where:\n",
    "- $ \\sum_{i=1}^n w_i^2 $ is the sum of the squares of the coefficients.\n",
    "- $ \\lambda $ is a constant that controls the strength of the penalty. A larger $ \\lambda $ makes the penalty stronger, which keeps the coefficients smaller.\n",
    "\n",
    "\n",
    "1. **Squaring values**: In Ridge regression, instead of using absolute values, you square each coefficient $ w_i $. Squaring a number makes sure it's non-negative (because $ w_i^2 \\geq 0 $), and larger values are penalized more heavily. For example, $ 3^2 = 9 $ and $ (-3)^2 = 9 $.\n",
    "\n",
    "2. **Summing squared coefficients**: You add up all the squared values of the coefficients, which is a measure of the model's complexity. Larger sums mean more complexity, and Ridge regression tries to minimize this.\n",
    "\n",
    "3. **Penalizing large coefficients**: By adding this penalty term to the cost, Ridge regression discourages large coefficients. This makes the model simpler and less prone to overfitting. However, unlike L1 regularization (Lasso), Ridge regression does not force coefficients to exactly zero. It just makes them smaller.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Imagine you have a linear model:\n",
    "$$\n",
    "y = w_1 x_1 + w_2 x_2 + b\n",
    "$$\n",
    "Normally, you’d minimize the error, but with Ridge regression, you also add the penalty:\n",
    "$$\n",
    "\\text{Cost} = \\text{Error} + \\lambda (w_1^2 + w_2^2)\n",
    "$$\n",
    "If $ w_1 $ or $ w_2 $ gets too large, the total cost will increase because of the squared values. This encourages the model to keep $ w_1 $ and $ w_2 $ smaller, but it won’t force them to zero like L1 regularization.\n",
    "\n",
    "#### Why is it useful?\n",
    "Ridge regression helps reduce overfitting by discouraging large coefficients in the model. It’s useful when you have a lot of variables, and you want to keep the model more general without forcing any coefficients to zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80691523-35fe-4382-bbaa-1deecab5a697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(X_train, y_train, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Trains a Ridge Regression model with L2 regularization.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (np.ndarray): Scaled training features.\n",
    "    - y_train (pd.Series or np.ndarray): Training target.\n",
    "    - alpha (float): Regularization strength.\n",
    "\n",
    "    Returns:\n",
    "    - ridge_model (Ridge): Trained Ridge Regression model.\n",
    "    \"\"\"\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    return ridge\n",
    "\n",
    "\n",
    "def ridge_polynomial_regression(X_train, y_train, degree = 2, alpha=1.0):\n",
    "    \n",
    "    ridge_pipeline = Pipeline([\n",
    "        ('poly_features', PolynomialFeatures(degree=degree, include_bias=False)),  \n",
    "        ('ridge', Ridge(alpha=alpha))\n",
    "    ])\n",
    "    \n",
    "    ridge_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    return ridge_pipeline\n",
    "\n",
    "\n",
    "def lasso_polynomial_regression(X_train, y_train, degree = 2, alpha=1.0):\n",
    "    \n",
    "    lasso_pipeline = Pipeline([\n",
    "        ('poly_features', PolynomialFeatures(degree=degree, include_bias=False)),  \n",
    "        ('lasso', Lasso(alpha=alpha))\n",
    "    ])\n",
    "    \n",
    "    lasso_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    return lasso_pipeline\n",
    "\n",
    "\n",
    "def elastic_net_regression(X_train, y_train, alpha=0.1, l1_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Trains an Elastic Net Regression model combining L1 and L2 regularization.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (np.ndarray): Scaled training features.\n",
    "    - y_train (pd.Series or np.ndarray): Training target.\n",
    "    - alpha (float): Regularization strength.\n",
    "    - l1_ratio (float): The ElasticNet mixing parameter, with 0 <= l1_ratio <= 1.\n",
    "                       l1_ratio=0 corresponds to Ridge, l1_ratio=1 to Lasso.\n",
    "\n",
    "    Returns:\n",
    "    - enet_model (ElasticNet): Trained Elastic Net Regression model.\n",
    "    \"\"\"\n",
    "    enet = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "    enet.fit(X_train, y_train)\n",
    "    return enet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "008b489e-b615-4898-8ae6-46b6c85669f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Trains and evaluates Multiple Linear Regression, Polynomial Regression,\n",
    "    Lasso Regression, Ridge Regression, and Elastic Net Regression models.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (np.ndarray): Scaled training features.\n",
    "    - y_train (pd.Series or np.ndarray): Training target.\n",
    "    - X_test (np.ndarray): Scaled testing features.\n",
    "    - y_test (pd.Series or np.ndarray): Testing target.\n",
    "\n",
    "    Returns:\n",
    "    - results (pd.DataFrame): DataFrame containing performance metrics for each model.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Multiple Linear Regression\n",
    "    lin_reg = multiple_linear_regression(X_train, y_train)\n",
    "    lin_metrics = evaluate_model(lin_reg, X_test, y_test, 'Multiple Linear Regression')\n",
    "    results['Multiple Linear Regression'] = lin_metrics\n",
    "    \n",
    "    # Polynomial Regression (Degree 2)\n",
    "    poly_reg = polynomial_regression(X_train, y_train, degree=2)\n",
    "    poly_metrics = evaluate_model(poly_reg, X_test, y_test, 'Polynomial Regression (Degree 2)')\n",
    "    results['Polynomial Regression (Degree 2)'] = poly_metrics\n",
    "    \n",
    "    # Lasso Regression\n",
    "    lasso = lasso_regression(X_train, y_train, alpha=0.1)\n",
    "    lasso_metrics = evaluate_model(lasso, X_test, y_test, 'Lasso Regression (L1)')\n",
    "    results['Lasso Regression (linear) (L1)'] = lasso_metrics\n",
    "\n",
    "\n",
    "    # Lasso Polynomial Regression\n",
    "    lasso_p = lasso_polynomial_regression(X_train, y_train, degree = 2, alpha=1.0)\n",
    "    lasso_p_metrics = evaluate_model(lasso_p, X_test, y_test, 'Lasso Regression (L1)')\n",
    "    results['Lasso Polynomial Regression (L1)'] = lasso_p_metrics\n",
    "    \n",
    "    \n",
    "    # Ridge Regression\n",
    "    ridge = ridge_regression(X_train, y_train, alpha=0.1)\n",
    "    ridge_metrics = evaluate_model(ridge, X_test, y_test, 'Ridge Regression (L2)')\n",
    "    results['Ridge Regression (linear) (L2)'] = ridge_metrics\n",
    "    \n",
    "    # Ridge Polynomial Regression\n",
    "    ridge_p = ridge_polynomial_regression(X_train, y_train, degree = 2, alpha=1000.0)\n",
    "    ridge_p_metrics = evaluate_model(ridge_p, X_test, y_test, 'Ridge Regression (L2)')\n",
    "    results['Ridge Polynomial Regression (L2)'] = ridge_p_metrics\n",
    "    \n",
    "    \n",
    "    # Elastic Net Regression\n",
    "    enet = elastic_net_regression(X_train, y_train, alpha=0.1, l1_ratio=0.5)\n",
    "    enet_metrics = evaluate_model(enet, X_test, y_test, 'Elastic Net Regression')\n",
    "    results['Elastic Net Regression'] = enet_metrics\n",
    "    \n",
    "    # Create a DataFrame for comparison\n",
    "    performance_df = pd.DataFrame(results).T\n",
    "    return performance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "959232c4-954a-4564-930a-fd0d3bef4fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_regression_analysis(file_path, target_column='Potability', test_size=0.2, random_state=42, poly_degree=2, lasso_alpha=0.1, ridge_alpha=1.0, enet_alpha=0.1, enet_l1_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Runs the full regression analysis including Multiple Linear Regression,\n",
    "    Polynomial Regression, and Regularization techniques on the specified dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the CSV file.\n",
    "    - target_column (str): Name of the target column.\n",
    "    - test_size (float): Proportion of the dataset to include in the test split.\n",
    "    - random_state (int): Controls the shuffling applied to the data before splitting.\n",
    "    - poly_degree (int): Degree of the polynomial for Polynomial Regression.\n",
    "    - lasso_alpha (float): Regularization strength for Lasso Regression.\n",
    "    - ridge_alpha (float): Regularization strength for Ridge Regression.\n",
    "    - enet_alpha (float): Regularization strength for Elastic Net Regression.\n",
    "    - enet_l1_ratio (float): The ElasticNet mixing parameter.\n",
    "\n",
    "    Returns:\n",
    "    - performance_df (pd.DataFrame): DataFrame containing performance metrics for each model.\n",
    "    - models (dict): Dictionary containing trained models.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = load_data(file_path)\n",
    "    print(\"Data Loaded Successfully.\\n\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    df_clean = handle_missing_values(df)\n",
    "    print(\"Missing Values Handled.\\n\")\n",
    "    \n",
    "    # Split dataset\n",
    "    X_train, X_test, y_train, y_test = split_dataset(df_clean, target_column, test_size, random_state)\n",
    "    print(\"Dataset Split into Training and Testing Sets.\\n\")\n",
    "    \n",
    "    # Scale features\n",
    "    X_train_scaled, X_test_scaled = scale_features(X_train, X_test)\n",
    "    print(\"Features Scaled.\\n\")\n",
    "\n",
    "    # Scale targets\n",
    "    y_train_scaled, y_test_scaled = scale_features(y_train.to_frame(), y_test.to_frame())\n",
    "    y_train_scaled = y_train_scaled.ravel()\n",
    "    y_test_scaled = y_test_scaled.ravel()\n",
    "    \n",
    "    # Compare models\n",
    "    performance_df = compare_models(X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n",
    "    print(\"Model Comparison Completed.\\n\")\n",
    "    \n",
    "    # Train Regularized Models separately if needed\n",
    "    models = {\n",
    "        'Multiple Linear Regression': multiple_linear_regression(X_train_scaled, y_train),\n",
    "        'Polynomial Regression': polynomial_regression(X_train_scaled, y_train, degree=poly_degree),\n",
    "        'Lasso Regression': lasso_regression(X_train_scaled, y_train, alpha=lasso_alpha),\n",
    "        'Ridge Regression': ridge_regression(X_train_scaled, y_train, alpha=ridge_alpha),\n",
    "        'Elastic Net Regression': elastic_net_regression(X_train_scaled, y_train, alpha=enet_alpha, l1_ratio=enet_l1_ratio)\n",
    "    }\n",
    "    \n",
    "    return performance_df, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f383c3c-87c0-4b0c-b289-ef737509fee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded Successfully.\n",
      "\n",
      "Missing Values in Each Column:\n",
      "ph                 491\n",
      "Hardness             0\n",
      "Solids               0\n",
      "Chloramines          0\n",
      "Sulfate            781\n",
      "Conductivity         0\n",
      "Organic_carbon       0\n",
      "Trihalomethanes    162\n",
      "Turbidity            0\n",
      "Potability           0\n",
      "dtype: int64\n",
      "Missing Values Handled.\n",
      "\n",
      "Dataset Split into Training and Testing Sets.\n",
      "\n",
      "Features Scaled.\n",
      "\n",
      "Multiple Linear Regression Performance:\n",
      "Mean Squared Error (MSE): 0.9916\n",
      "Mean Absolute Error (MAE): 0.7821\n",
      "R² Score: 0.0008\n",
      "\n",
      "Polynomial Regression (Degree 2) Performance:\n",
      "Mean Squared Error (MSE): 1.0191\n",
      "Mean Absolute Error (MAE): 0.7930\n",
      "R² Score: -0.0269\n",
      "\n",
      "Lasso Regression (L1) Performance:\n",
      "Mean Squared Error (MSE): 0.9931\n",
      "Mean Absolute Error (MAE): 0.7854\n",
      "R² Score: -0.0007\n",
      "\n",
      "Lasso Regression (L1) Performance:\n",
      "Mean Squared Error (MSE): 0.9931\n",
      "Mean Absolute Error (MAE): 0.7854\n",
      "R² Score: -0.0007\n",
      "\n",
      "Ridge Regression (L2) Performance:\n",
      "Mean Squared Error (MSE): 0.9916\n",
      "Mean Absolute Error (MAE): 0.7821\n",
      "R² Score: 0.0008\n",
      "\n",
      "Ridge Regression (L2) Performance:\n",
      "Mean Squared Error (MSE): 1.0006\n",
      "Mean Absolute Error (MAE): 0.7857\n",
      "R² Score: -0.0082\n",
      "\n",
      "Elastic Net Regression Performance:\n",
      "Mean Squared Error (MSE): 0.9931\n",
      "Mean Absolute Error (MAE): 0.7854\n",
      "R² Score: -0.0007\n",
      "\n",
      "Model Comparison Completed.\n",
      "\n",
      "Model Performance Comparison:\n",
      "                                       MSE       MAE        R²\n",
      "Multiple Linear Regression        0.991628  0.782077  0.000819\n",
      "Polynomial Regression (Degree 2)  1.019095  0.793043 -0.026858\n",
      "Lasso Regression (linear) (L1)    0.993132  0.785379 -0.000697\n",
      "Lasso Polynomial Regression (L1)  0.993132  0.785379 -0.000697\n",
      "Ridge Regression (linear) (L2)    0.991628  0.782077  0.000819\n",
      "Ridge Polynomial Regression (L2)  1.000557  0.785743 -0.008178\n",
      "Elastic Net Regression            0.993132  0.785379 -0.000697\n"
     ]
    }
   ],
   "source": [
    "file_path = '../dataset/water_potability.csv'  \n",
    "target_column = 'Turbidity'\n",
    "\n",
    "performance_df, trained_models = run_full_regression_analysis(\n",
    "    file_path=file_path,\n",
    "    target_column=target_column,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    poly_degree=2,\n",
    "    lasso_alpha=0.1,\n",
    "    ridge_alpha=1.0,\n",
    "    enet_alpha=0.1,\n",
    "    enet_l1_ratio=0.5\n",
    ")\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(performance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d879fdbf-9393-4803-9acf-2e4671b28dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with mean imputation\n",
    "\n",
    "# Model Performance Comparison:\n",
    "#                                          MSE        MAE        R²\n",
    "# Multiple Linear Regression        239.455271  11.964480 -0.002330\n",
    "# Polynomial Regression (Degree 2)  241.377260  12.135437 -0.010375\n",
    "# Lasso Regression (L1)             239.274149  11.943337 -0.001572\n",
    "# Ridge Regression (L2)             239.454914  11.964451 -0.002329\n",
    "# Elastic Net Regression            239.386005  11.951843 -0.002040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b7afe00-d025-4155-8614-e2942f81b5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ph</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>Solids</th>\n",
       "      <th>Chloramines</th>\n",
       "      <th>Sulfate</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>Organic_carbon</th>\n",
       "      <th>Trihalomethanes</th>\n",
       "      <th>Turbidity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>204.890455</td>\n",
       "      <td>20791.318981</td>\n",
       "      <td>7.300212</td>\n",
       "      <td>368.516441</td>\n",
       "      <td>564.308654</td>\n",
       "      <td>10.379783</td>\n",
       "      <td>86.990970</td>\n",
       "      <td>2.963135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.716080</td>\n",
       "      <td>129.422921</td>\n",
       "      <td>18630.057858</td>\n",
       "      <td>6.635246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>592.885359</td>\n",
       "      <td>15.180013</td>\n",
       "      <td>56.329076</td>\n",
       "      <td>4.500656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.099124</td>\n",
       "      <td>224.236259</td>\n",
       "      <td>19909.541732</td>\n",
       "      <td>9.275884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>418.606213</td>\n",
       "      <td>16.868637</td>\n",
       "      <td>66.420093</td>\n",
       "      <td>3.055934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.316766</td>\n",
       "      <td>214.373394</td>\n",
       "      <td>22018.417441</td>\n",
       "      <td>8.059332</td>\n",
       "      <td>356.886136</td>\n",
       "      <td>363.266516</td>\n",
       "      <td>18.436524</td>\n",
       "      <td>100.341674</td>\n",
       "      <td>4.628771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.092223</td>\n",
       "      <td>181.101509</td>\n",
       "      <td>17978.986339</td>\n",
       "      <td>6.546600</td>\n",
       "      <td>310.135738</td>\n",
       "      <td>398.410813</td>\n",
       "      <td>11.558279</td>\n",
       "      <td>31.997993</td>\n",
       "      <td>4.075075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3271</th>\n",
       "      <td>4.668102</td>\n",
       "      <td>193.681735</td>\n",
       "      <td>47580.991603</td>\n",
       "      <td>7.166639</td>\n",
       "      <td>359.948574</td>\n",
       "      <td>526.424171</td>\n",
       "      <td>13.894419</td>\n",
       "      <td>66.687695</td>\n",
       "      <td>4.435821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3272</th>\n",
       "      <td>7.808856</td>\n",
       "      <td>193.553212</td>\n",
       "      <td>17329.802160</td>\n",
       "      <td>8.061362</td>\n",
       "      <td>NaN</td>\n",
       "      <td>392.449580</td>\n",
       "      <td>19.903225</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.798243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3273</th>\n",
       "      <td>9.419510</td>\n",
       "      <td>175.762646</td>\n",
       "      <td>33155.578218</td>\n",
       "      <td>7.350233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>432.044783</td>\n",
       "      <td>11.039070</td>\n",
       "      <td>69.845400</td>\n",
       "      <td>3.298875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>5.126763</td>\n",
       "      <td>230.603758</td>\n",
       "      <td>11983.869376</td>\n",
       "      <td>6.303357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>402.883113</td>\n",
       "      <td>11.168946</td>\n",
       "      <td>77.488213</td>\n",
       "      <td>4.708658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3275</th>\n",
       "      <td>7.874671</td>\n",
       "      <td>195.102299</td>\n",
       "      <td>17404.177061</td>\n",
       "      <td>7.509306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>327.459760</td>\n",
       "      <td>16.140368</td>\n",
       "      <td>78.698446</td>\n",
       "      <td>2.309149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3276 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ph    Hardness        Solids  Chloramines     Sulfate  \\\n",
       "0          NaN  204.890455  20791.318981     7.300212  368.516441   \n",
       "1     3.716080  129.422921  18630.057858     6.635246         NaN   \n",
       "2     8.099124  224.236259  19909.541732     9.275884         NaN   \n",
       "3     8.316766  214.373394  22018.417441     8.059332  356.886136   \n",
       "4     9.092223  181.101509  17978.986339     6.546600  310.135738   \n",
       "...        ...         ...           ...          ...         ...   \n",
       "3271  4.668102  193.681735  47580.991603     7.166639  359.948574   \n",
       "3272  7.808856  193.553212  17329.802160     8.061362         NaN   \n",
       "3273  9.419510  175.762646  33155.578218     7.350233         NaN   \n",
       "3274  5.126763  230.603758  11983.869376     6.303357         NaN   \n",
       "3275  7.874671  195.102299  17404.177061     7.509306         NaN   \n",
       "\n",
       "      Conductivity  Organic_carbon  Trihalomethanes  Turbidity  \n",
       "0       564.308654       10.379783        86.990970   2.963135  \n",
       "1       592.885359       15.180013        56.329076   4.500656  \n",
       "2       418.606213       16.868637        66.420093   3.055934  \n",
       "3       363.266516       18.436524       100.341674   4.628771  \n",
       "4       398.410813       11.558279        31.997993   4.075075  \n",
       "...            ...             ...              ...        ...  \n",
       "3271    526.424171       13.894419        66.687695   4.435821  \n",
       "3272    392.449580       19.903225              NaN   2.798243  \n",
       "3273    432.044783       11.039070        69.845400   3.298875  \n",
       "3274    402.883113       11.168946        77.488213   4.708658  \n",
       "3275    327.459760       16.140368        78.698446   2.309149  \n",
       "\n",
       "[3276 rows x 9 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a4665-9ba4-4001-a1ac-7faf5dc82fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
